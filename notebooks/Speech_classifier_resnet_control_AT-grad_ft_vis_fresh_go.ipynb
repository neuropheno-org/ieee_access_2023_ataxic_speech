{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabb433d",
   "metadata": {},
   "source": [
    "# Speech classifier for NDs using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d456f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "from scipy.stats import shapiro,normaltest,kstest,uniform\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "sys.path.append('../../')\n",
    "\n",
    "#sklearn \n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score, roc_curve,auc, roc_auc_score,ConfusionMatrixDisplay\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch. optim.lr_scheduler import _LRScheduler\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#Pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from pytorch_lightning import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "#models\n",
    "from script.models import CNN_short_fc_wide, FC_Resnet_\n",
    "\n",
    "#utils\n",
    "from script.utils import KFoldCVDataModule, CVTrainer, PadImage, ImbalancedDatasetSampler\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "#Captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62014d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(42)\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definition\n",
    "epochs = 60 # no of epochs\n",
    "model_size_ = '18'\n",
    "Batch_Size = 128 #batch size\n",
    "no_feutures = 128 #no of features per entry\n",
    "no_classes = 2 #no of classes to classify \n",
    "training_on = False\n",
    "root_dir = '/home/kvattis/Documents/data/'\n",
    "#train_csv_file = root_dir + 'train_dataset_control_AT_Cookie_Theft_v0.csv'\n",
    "#val_csv_file = root_dir + 'val_dataset_control_AT_Cookie_Theft_v0.csv'\n",
    "#train_csv_file = root_dir + 'train_dataset_control_AT_MOY_v0.csv'\n",
    "#val_csv_file = root_dir + 'val_dataset_control_AT_MOY_v0.csv'\n",
    "train_csv_file = root_dir + 'train_dataset_control_AT_Mel_Spec_2022_noise_red_v0.csv'\n",
    "val_csv_file = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red_v0.csv'\n",
    "parent_directory = '/home/kvattis/Documents/speech_analysis/'\n",
    "checkpoint_directory = parent_directory + 'checkpoints/resnet_class_fresh/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = [1771,3952]\n",
    "weights = [1/x for x in n_class]\n",
    "weights = [ww/np.sum(weights) for ww in weights]\n",
    "#weights = [0.65, 0.35]\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e837fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(X, range_=(0, 1)):\n",
    "    mi, ma = range_\n",
    "    X_min = -50\n",
    "    X_max = 50\n",
    "    #X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_std = (X - X_min) / (X_max - X_min)\n",
    "    X_scaled = X_std * (ma - mi) + mi\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butterLow(cutoff, critical, order):\n",
    "    normal_cutoff = float(cutoff) / critical\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='lowpass')\n",
    "    return b, a\n",
    "\n",
    "def butterFilter(data, cutoff_freq, nyq_freq, order):\n",
    "    b, a = butterLow(cutoff_freq, nyq_freq, order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting standard filter requirements.\n",
    "order = 6\n",
    "nyq_freq = 30.0       \n",
    "cutoff_frequency = 3.667#5.5#3.667  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augm(spec):\n",
    "    freq_mask_param = 25\n",
    "    time_mask_param = 10\n",
    "    \n",
    "    masking_T = T.TimeMasking(time_mask_param=time_mask_param)\n",
    "    masking_f = T.FrequencyMasking(freq_mask_param = freq_mask_param)\n",
    "\n",
    "    spec = masking_T(spec)\n",
    "    spec = masking_f(spec)\n",
    "    \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    \n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e417d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd86d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain(spec):\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c97e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_mean(value:torch.Tensor, labels:torch.LongTensor) -> (torch.Tensor, torch.LongTensor):\n",
    "    \"\"\"Group-wise average for (sparse) grouped tensors\n",
    "\n",
    "    Args:\n",
    "        value (torch.Tensor): values to average (# samples, latent dimension)\n",
    "        labels (torch.LongTensor): labels for embedding parameters (# samples,)\n",
    "\n",
    "    Returns: \n",
    "        result (torch.Tensor): (# unique labels, latent dimension)\n",
    "        new_labels (torch.LongTensor): (# unique labels,)\n",
    "\n",
    "    Examples:\n",
    "        >>> samples = torch.Tensor([\n",
    "                             [0.15, 0.15, 0.15],    #-> group / class 1\n",
    "                             [0.2, 0.2, 0.2],    #-> group / class 3\n",
    "                             [0.4, 0.4, 0.4],    #-> group / class 3\n",
    "                             [0.0, 0.0, 0.0]     #-> group / class 0\n",
    "                      ])\n",
    "        >>> labels = torch.LongTensor([1, 5, 5, 0])\n",
    "        >>> result, new_labels = groupby_mean(samples, labels)\n",
    "\n",
    "        >>> result\n",
    "        tensor([[0.0000, 0.0000, 0.0000],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.3000, 0.3000, 0.3000]])\n",
    "\n",
    "        >>> new_labels\n",
    "        tensor([0, 1, 5])\n",
    "    \"\"\"\n",
    "    uniques = labels.unique().tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    key_val = {key: val for key, val in zip(uniques, range(len(uniques)))}\n",
    "    val_key = {val: key for key, val in zip(uniques, range(len(uniques)))}\n",
    "\n",
    "    labels = torch.LongTensor(list(map(key_val.get, labels)))\n",
    "\n",
    "    labels = labels.view(labels.size(0), 1).expand(-1, value.size(1))\n",
    "\n",
    "    unique_labels, labels_count = labels.unique(dim=0, return_counts=True)\n",
    "    result = torch.zeros_like(unique_labels.to(device), dtype=value.dtype).scatter_add_(0, labels.to(device), value.to(device))\n",
    "    result = result.to(device) / labels_count.float().unsqueeze(1).to(device)\n",
    "    new_labels = torch.LongTensor(list(map(val_key.get, unique_labels[:, 0].tolist())))\n",
    "    return result.to(device), new_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms_train(spec,l):\n",
    "    upper_limit = spec.shape[1]\n",
    "    if l == 0:\n",
    "        random_size = random.randint(25,upper_limit)\n",
    "        transforms_ = transforms.Compose([transforms.RandomCrop((random_size, 128)),transforms.Resize((100, 100))])\n",
    "    else:\n",
    "        random_size = random.randint(35,upper_limit)\n",
    "        transforms_ = transforms.Compose([transforms.RandomCrop((random_size, 128)),transforms.Resize((100, 100))])\n",
    "    spec = transforms_(spec)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms_val(spec,l):\n",
    "    transforms_resize = transforms.Resize((100, 100))\n",
    "    spec = transforms_resize(spec)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_std(X, mean = -0.0005, std = 0.0454):\n",
    "    X_scaled = (X - mean)/ std\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a pytorch Dataset        \n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, csv_file,root_dir,transform, flag = 't'):\n",
    "            \n",
    "        self.file_names = pd.read_csv(csv_file,header = None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        address =  os.path.join(self.root_dir,\n",
    "                                self.file_names.iloc[idx, 2])\n",
    "                \n",
    "        df = pd.read_csv(address,header = None)                                                                              \n",
    "        df_ar = df.to_numpy()\n",
    "        df_ar = min_max_scale(df_ar)\n",
    "        df_ar_mel = df_ar\n",
    "        df_ar_mel = torch.Tensor(df_ar_mel)\n",
    "        \n",
    "        df_ar_t = np.gradient(df_ar, axis = 0)\n",
    "        #df_ar_t = butterFilter(df_ar_t, cutoff_frequency, nyq_freq/2., order = order)\n",
    "        #df_ar_t_p = np.where(df_ar_t > 0, df_ar_t, 0)\n",
    "        #df_ar_t_n = np.abs(np.where(df_ar_t < 0, df_ar_t, 0))\n",
    "        \n",
    "        df_ar_f = np.gradient(df_ar, axis = 1)\n",
    "        #df_ar_f = butterFilter(df_ar_f, cutoff_frequency, nyq_freq/2., order = order)\n",
    "        #df_ar_f_p = np.where(df_ar_f > 0, df_ar_f, 0)\n",
    "        #df_ar_f_n = np.abs(np.where(df_ar_f < 0, df_ar_f, 0))\n",
    "\n",
    "        #df_ar = np.stack((df_ar_t_p,df_ar_t_n,df_ar_f_p,df_ar_f_n), axis=0)\n",
    "        df_ar = np.stack((df_ar_t,df_ar_f), axis=0)#np.stack((df_ar_t_p,df_ar_t_n,df_ar_f_p,df_ar_f_n), axis=0)\n",
    "        \n",
    "        df_ar =  global_std(df_ar)\n",
    "        data = torch.Tensor(df_ar)\n",
    "        #df_ar_t = global_std(df_ar_t)\n",
    "        #data = torch.Tensor(df_ar_t)\n",
    "        label_ = self.file_names.iloc[idx, 3]\n",
    "        label = torch.LongTensor([label_])\n",
    "        p_id = self.file_names.iloc[idx, 1]\n",
    "        #p_id = torch.LongTensor([p_id])\n",
    "        adr_id = int(str(p_id) + str(self.file_names.iloc[idx, 4]))\n",
    "        adr_id = torch.LongTensor([adr_id])\n",
    "\n",
    "        #data = torch.unsqueeze(data, 0)\n",
    "        df_ar_mel = torch.unsqueeze(df_ar_mel, 0)\n",
    "        if self.transform:\n",
    "            data = self.transform(data,label_)\n",
    "            df_ar_mel = self.transform(df_ar_mel,label_)\n",
    "            \n",
    "        \n",
    "        return data, label, adr_id, df_ar_mel #[label, p_id] #torch.cat([data,data,data], dim = 0), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataModule to create the datasets and the dataloaders\n",
    "class SpeechDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,train_dataset, test_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.dataloader_kwargs = {'batch_size' : self.batch_size,\n",
    "                             'shuffle' : True,\n",
    "                             'num_workers' : 4,\n",
    "                             'collate_fn' : PadImage()}\n",
    "        #y_train = []\n",
    "        \n",
    "        #for i in range(len(train_dataset)):\n",
    "         #   y_train.append(train_dataset[i][1].item())\n",
    "            \n",
    "        #y_train = np.array(y_train)\n",
    "        \n",
    "        #class_sample_count = np.array([len(np.where(y_train==t)[0]) for t in np.unique(y_train)])\n",
    "        #weight = 1. / class_sample_count\n",
    "        #samples_weight = np.array([weight[t] for t in y_train])\n",
    "\n",
    "        #samples_weight = torch.from_numpy(samples_weight)\n",
    "        #self.sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "        \n",
    "    def setup(self,stage=None):\n",
    "        self.train_dataset = self.train_dataset # ImbalancedDatasetSampler(self.train_dataset) sampler = ImbalancedDatasetSampler(self.test_dataset)\n",
    "        self.test_dataset = self.test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, sampler = ImbalancedDatasetSampler(self.train_dataset), shuffle = False, batch_size = self.batch_size, num_workers = 8, collate_fn=PadImage())\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = len(self.test_dataset), shuffle = False, num_workers = 8, collate_fn=PadImage())\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset , batch_size = self.batch_size, shuffle = False, num_workers = 8, collate_fn=PadImage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3bd967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the module  \n",
    "train_dataset = SpeechDataset(train_csv_file, root_dir,transforms_train)\n",
    "test_dataset = SpeechDataset(val_csv_file, root_dir,transforms_val)\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "data_module = SpeechDataModule(train_dataset, test_dataset, Batch_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9259b31",
   "metadata": {},
   "source": [
    "librosa.display.specshow((next(iter(data_module.val_dataloader()))[0][4][0].numpy().T), x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[43][2][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86195581",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[43][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78211a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(train_dataset[10][0][0].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(train_dataset[10][0][0].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(train_dataset[10][3][0].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41327bcc",
   "metadata": {},
   "source": [
    "librosa.display.specshow(train_dataset[10][0][3].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(train_dataset[11][0][0].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(train_dataset[2][0][0].numpy().T, x_axis='time', sr=8000,hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(test_dataset[10][0][1].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fa628",
   "metadata": {},
   "source": [
    "librosa.display.specshow(test_dataset[10][0][1].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6458e37",
   "metadata": {},
   "source": [
    "librosa.display.specshow(test_dataset[10][0][2].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b8fa0",
   "metadata": {},
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "max_ = -10000\n",
    "min_ = 10000\n",
    "for data in data_module.train_dataloader():\n",
    "    data = data[0]\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    if data.max() > max_:\n",
    "        max_ = data.max()\n",
    "        \n",
    "    if data.min() < min_:\n",
    "        min_ = data.min()\n",
    "        \n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "print(mean)\n",
    "print(std)\n",
    "print(max_)\n",
    "print(min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor class performing all the calculations for loss, backpropagation etc        \n",
    "class Speech_Predictor(pl.LightningModule):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super(Speech_Predictor,self).__init__()\n",
    "        self.model = FC_Resnet_(num_layers = 2, num_classes = n_classes) #CNN_short_fc_wide(n_classes=n_classes, n_channels = 1) #FC_Resnet_(num_layers = 1, num_classes = n_classes)#CNN_short_fc(n_classes=n_classes) #FCNN_short(n_classes=n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()#weight = class_weights)\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "        self.train_f1 = torchmetrics.F1(num_classes = n_classes, average = 'weighted')\n",
    "        self.valid_f1 = torchmetrics.F1(num_classes = n_classes, average = 'weighted')\n",
    "        self.test_f1 = torchmetrics.F1(num_classes = n_classes, average = 'weighted')\n",
    "        self.train_f1_class = torchmetrics.F1(num_classes = n_classes, average = None)\n",
    "        self.valid_f1_class = torchmetrics.F1(num_classes = n_classes, average = None)\n",
    "        self.test_f1_class = torchmetrics.F1(num_classes = n_classes, average = None)\n",
    "        self.train_auc_class = torchmetrics.AUROC(num_classes = n_classes, average = None)\n",
    "        self.valid_auc_class = torchmetrics.AUROC(num_classes = n_classes, average = None)\n",
    "        self.test_auc_class = torchmetrics.AUROC(num_classes = n_classes, average = None)\n",
    "        self.n_classes_ = n_classes\n",
    "        \n",
    "    def forward(self,x,labels = None, targets_a = None, targets_b = None, lam = None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            if lam is not None:\n",
    "                loss =  mixup_criterion(self.criterion, output, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                loss = self.criterion(output,labels)\n",
    "            return loss, output\n",
    "        else:\n",
    "            #output = F.log_softmax(output,dim =1)\n",
    "            output = F.softmax(output,dim =1)\n",
    "            return output\n",
    "        \n",
    "        \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "        \n",
    "        #loss, outputs = self(torch.squeeze(X, 1),y)\n",
    "        #outputs = F.softmax(outputs,dim =1)\n",
    "        #yhat = torch.argmax(outputs, dim =1)\n",
    "        #self.train_acc(yhat, y)\n",
    "        #train_f1 = self.train_f1(yhat, y)\n",
    "        #train_f1_class = self.train_f1_class(yhat, y)\n",
    "        #train_auc_class = self.train_auc_class(outputs, y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        X, y_a, y_b, lam = mixup_data(X, y, alpha = 0.1)\n",
    "        X, y_a, y_b = map(Variable, ( X, y_a, y_b))\n",
    "        loss, outputs = self(x = X,labels = y, targets_a = y_a, targets_b = y_b,lam = lam)\n",
    "        outputs = F.softmax(outputs,dim =1)\n",
    "        yhat = torch.argmax(outputs, dim =1)\n",
    "        #self.train_acc(yhat, y)\n",
    "        train_f1 = lam * self.train_f1(yhat, y_a) + (1 - lam) * self.train_f1(yhat, y_b)\n",
    "        train_f1_class = lam * self.train_f1_class(yhat, y_a) + (1 - lam) * self.train_f1_class(yhat, y_b) \n",
    "        #train_auc_class = lam * np.array(self.train_auc_class(outputs, y_a)) + (1 - lam) * np.array(self.train_auc_class(outputs, y_b)) \n",
    "        \n",
    "        \n",
    "        self.log(\"train_loss\",loss,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"train_accuracy\",self.train_acc,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_f1\",train_f1,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_f1_control\",train_f1_class[0],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_f1_AT\",train_f1_class[1],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"train_f1_PD\",train_f1_class[2],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"train_auc_control\",train_auc_class[0],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"train_auc_AT\",train_auc_class[1],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"train_auc_PD\",train_auc_class[2],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return {\"loss\": loss, \"accuracy\": self.train_acc}\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "        i_d = batch[2]\n",
    "        loss, outputs = self(x = X, labels = y)\n",
    "        outputs = F.softmax(outputs,dim =1)\n",
    "        outputs, _ = groupby_mean(outputs, i_d)\n",
    "        yhat = torch.argmax(outputs, dim =1)\n",
    "        y, y_index = groupby_mean(y.view((y.shape[0],1)), i_d)\n",
    "        y = y.view((y.shape[0])).type(torch.LongTensor).to(device)\n",
    "        #self.valid_acc(yhat, y)\n",
    "        valid_f1 = self.valid_f1(yhat, y)\n",
    "        valid_f1_class = self.valid_f1_class(yhat, y)\n",
    "        valid_auc_class = self.valid_auc_class(outputs, y)\n",
    "        \n",
    "        loss = self.criterion(outputs,y)\n",
    "        #self.log(\"val_loss\",loss,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        self.log(\"val_loss\",loss,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"val_accuracy\",self.valid_acc,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"val_f1\",valid_f1,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"val_f1_control\",valid_f1_class[0],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"val_f1_AT\",valid_f1_class[1],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"val_f1_PD\",valid_f1_class[2],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"val_auc_control\",valid_auc_class[0],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"val_auc_AT\",valid_auc_class[1],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"val_auc_PD\",valid_auc_class[2],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return {\"loss\": loss, \"accuracy\": self.valid_acc}\n",
    "    '''\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "        loss, outputs = self(torch.squeeze(X, 1),y)\n",
    "        outputs = F.softmax(outputs,dim =1)\n",
    "        yhat = torch.argmax(outputs, dim =1)\n",
    "        #self.test_acc(yhat,y)\n",
    "        test_f1 = self.test_f1(yhat,y)\n",
    "        test_f1_class = self.test_f1_class(yhat, y)\n",
    "        test_auc_class = self.test_auc_class(outputs, y)\n",
    "\n",
    "        self.log(\"test_loss\",loss,prog_bar = True, logger = True,on_step=True, on_epoch=True)\n",
    "        #self.log(\"test_accuracy\",self.test_acc,prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"test_f1\",test_f1,prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"test_f1_control\",test_f1_class[0],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"test_f1_AT\",test_f1_class[1],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"test_f1_PD\",test_f1_class[2],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"test_auc_control\",test_auc_class[0],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"test_auc_AT\",test_auc_class[1],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        #self.log(\"test_auc_PD\",test_auc_class[2],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return {\"loss\": loss, \"accuracy\": self.test_acc}\n",
    "    '''\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = optim.Adam(self.parameters(), lr =1e-2)\n",
    "        optimizer = optim.AdamW(self.parameters(), lr =1.e-4, weight_decay=1e-5)\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9,weight_decay= 0.1)\n",
    "\n",
    "\n",
    "        '''\n",
    "        lr_scheduler = {\n",
    "        'scheduler': optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-3 , epochs=50, anneal_strategy='linear'),\n",
    "        'name': 'SDG_lr',\n",
    "        'monitor': 'val_loss_epoch'}\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        lr_scheduler = {\n",
    "        'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10),\n",
    "        'name': 'SDG_lr',\n",
    "        'monitor': 'val_loss_epoch'}\n",
    "        \n",
    "        '''\n",
    "        lr_scheduler = {\n",
    "        'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 20),\n",
    "        'name': 'SDG_lr',\n",
    "        'monitor': 'val_loss_epoch'}\n",
    "        '''\n",
    "\n",
    "        return [optimizer]# , [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model       \n",
    "model = Speech_Predictor(n_classes = no_classes)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf24e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint and loger definition\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_directory,filename='Small_cnn_best-checkpoint-{epoch:02d}-{val_loss:.2f}_control_AT_Rep_con_grad_nr_v1',save_top_k=3, verbose =True , monitor = 'val_loss_epoch',mode ='min')\n",
    "logger = TensorBoardLogger(parent_directory + 'lightning_logs', name = 'Speech_small_cnn_control_AT_Mel_Rep_con_nr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_on is True:\n",
    "    #Defining the trainer object\n",
    "    trainer = pl.Trainer(logger = logger, callbacks = [checkpoint_callback], max_epochs = epochs, gpus = 1)\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370f6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a489cd6",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "checkpoint_loc_v0 = checkpoint_directory + 'Smallcnn_best-checkpoint-epoch=80-val_loss=0.45_control_AT__Mel_fresh_grad_tf_gotest_v0.ckpt'\n",
    "checkpoint_loc_v1 = checkpoint_directory + 'Smallcnn_best-checkpoint-epoch=93-val_loss=0.53_control_AT__Mel_grad_tf_fresh_gotest_v1.ckpt'\n",
    "checkpoint_loc_v2 = checkpoint_directory + 'Smallcnn_best-checkpoint-epoch=63-val_loss=0.47_control_AT__Mel_grad_tf_fresh_gotest_v2.ckpt'\n",
    "checkpoint_loc_v3 = checkpoint_directory + 'Smallcnn_best-checkpoint-epoch=57-val_loss=0.53_control_AT__Mel_grad_tf_fresh_gotest_v3.ckpt'\n",
    "checkpoint_loc_v4 = checkpoint_directory + 'Smallcnn_best-checkpoint-epoch=59-val_loss=0.48_control_AT__Mel_grad_tf_fresh_gotest_v4.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "trained_model_v0 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v0,n_classes = no_classes, model_size = model_size_)\n",
    "trained_model_v1 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v1,n_classes = no_classes, model_size = model_size_)\n",
    "trained_model_v2 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v2,n_classes = no_classes, model_size = model_size_)\n",
    "trained_model_v3 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v3,n_classes = no_classes, model_size = model_size_)\n",
    "trained_model_v4 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v4,n_classes = no_classes, model_size = model_size_)\n",
    "\n",
    "trained_model_v0.freeze()\n",
    "trained_model_v0.double()\n",
    "trained_model_v1.freeze()\n",
    "trained_model_v1.double()\n",
    "trained_model_v2.freeze()\n",
    "trained_model_v2.double()\n",
    "trained_model_v3.freeze()\n",
    "trained_model_v3.double()\n",
    "trained_model_v4.freeze()\n",
    "trained_model_v4.double()\n",
    "\n",
    "models = [trained_model_v0, trained_model_v1, trained_model_v2, trained_model_v3, trained_model_v4]\n",
    "\n",
    "#All validation data sets \n",
    "\n",
    "val_csv_file_v0 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_gotest_v0.csv'\n",
    "val_csv_file_v1 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_gotest_v1.csv'\n",
    "val_csv_file_v2 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_gotest_v2.csv'\n",
    "val_csv_file_v3 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_gotest_v3.csv'\n",
    "val_csv_file_v4 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_gotest_v4.csv'\n",
    "\n",
    "test_dataset_v0 = SpeechDataset(val_csv_file_v0, root_dir,transforms_val)\n",
    "test_dataset_v1 = SpeechDataset(val_csv_file_v1, root_dir,transforms_val)\n",
    "test_dataset_v2 = SpeechDataset(val_csv_file_v2, root_dir,transforms_val)\n",
    "test_dataset_v3 = SpeechDataset(val_csv_file_v3, root_dir,transforms_val)\n",
    "test_dataset_v4 = SpeechDataset(val_csv_file_v4, root_dir,transforms_val)\n",
    "\n",
    "all_data = [test_dataset_v0, test_dataset_v1, test_dataset_v2, test_dataset_v3, test_dataset_v4]\n",
    "\n",
    "#Demographics files\n",
    "\n",
    "val_demo_v0 = pd.read_csv(root_dir + 'val_demo_Mel_cnn_nr2_gotest_v0.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\",\"Bars_Speech\",\"Date\"])\n",
    "val_demo_v1 = pd.read_csv(root_dir + 'val_demo_Mel_cnn_nr2_gotest_v1.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v2 = pd.read_csv(root_dir + 'val_demo_Mel_cnn_nr2_gotest_v2.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v3 = pd.read_csv(root_dir + 'val_demo_Mel_cnn_nr2_gotest_v3.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v4 = pd.read_csv(root_dir + 'val_demo_Mel_cnn_nr2_gotest_v4.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "\n",
    "val_demo_ = [val_demo_v0, val_demo_v1, val_demo_v2, val_demo_v3, val_demo_v4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the output of the models\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    test_dataset = all_data[i]\n",
    "    trained_model = models[i]\n",
    "    val_demo = val_demo_[i]\n",
    "    prob_control = []\n",
    "    prob_AT = []\n",
    "    y_label_list =[]\n",
    "    y_prediction =[]\n",
    "    date_list = []\n",
    "    for sample in test_dataset:\n",
    "        X_s, y_label,date_, mel_spec = sample\n",
    "        if X_s.shape[1]< 50:\n",
    "            prob_control.append(np.nan)\n",
    "            prob_AT.append(np.nan)\n",
    "            y_label_list.append(np.nan)\n",
    "            y_prediction.append(np.nan)\n",
    "            date_list.append(np.nan)\n",
    "            continue\n",
    "        #print(y_label)\n",
    "        input_ = X_s.double().unsqueeze(0)\n",
    "        output = trained_model(input_)\n",
    "        prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "        prob_control.append(output[0][0].detach().cpu().numpy())\n",
    "        prob_AT.append(output[0][1].detach().cpu().numpy())\n",
    "        y_label_list.append(y_label[0].detach().cpu().numpy())\n",
    "        y_prediction.append(pred_label_idx[0][0].detach().cpu().numpy())\n",
    "        date_list.append(date_[0].detach().cpu().numpy())\n",
    "    val_demo[\"Prob_control\"] = prob_control\n",
    "    val_demo[\"Prob_AT\"] = prob_AT\n",
    "    val_demo[\"Prediction\"] = y_prediction\n",
    "    val_demo[\"Label\"] = y_label_list\n",
    "    val_demo[\"Prob_Ratio\"] = np.where(val_demo[\"Label\"] == 1 , val_demo[\"Prob_AT\"]/val_demo[\"Prob_control\"], val_demo[\"Prob_control\"]/val_demo[\"Prob_AT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo__ = [val_demo_[0], val_demo_[1], val_demo_[2], val_demo_[3], val_demo_[4]]\n",
    "val_demo_all = pd.concat(val_demo__, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb91b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_all = val_demo_all[val_demo_all['Prob_AT'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da64901",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['Bars','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars']= 0. \n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars'].notna()]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"].rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"].rank()-1\n",
    "#print(len(val_demo_bars[val_demo_bars['Label'] == 0]))    \n",
    "val_demo_bars_male = val_demo_bars_male[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).median()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "\n",
    "upper_quantile_male = val_demo_bars_male_upper68['Prob_AT'] - val_demo_bars_male['Prob_AT']\n",
    "lower_quantile_male = val_demo_bars_male['Prob_AT'] - val_demo_bars_male_lower68['Prob_AT']\n",
    "\n",
    "upper_quantile_female = val_demo_bars_female_upper68['Prob_AT'] - val_demo_bars_female['Prob_AT']\n",
    "lower_quantile_female = val_demo_bars_female['Prob_AT'] - val_demo_bars_female_lower68['Prob_AT']\n",
    "\n",
    "#plt.errorbar(val_demo_bars_male['Bars'], val_demo_bars_male['Prob_AT'], yerr=[lower_quantile_male.to_numpy(),upper_quantile_male.to_numpy()], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "#plt.errorbar(val_demo_bars_female['Bars'], val_demo_bars_female['Prob_AT'], yerr=[lower_quantile_female.to_numpy(),upper_quantile_female.to_numpy()], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "\n",
    "#plt.errorbar(val_demo_bars_male['Bars'], val_demo_bars_male['Prob_AT'], yerr = val_demo_bars_male_std['Prob_AT'], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "#plt.errorbar(val_demo_bars_female['Bars'], val_demo_bars_female['Prob_AT'], yerr = val_demo_bars_female_std['Prob_AT'], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "\n",
    "ax.scatter(val_demo_bars_male['Bars'], val_demo_bars_male['Prob_AT'], c = val_demo_bars_male['Label'],cmap=\"bwr\", marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "ax.scatter(val_demo_bars_female['Bars'], val_demo_bars_female['Prob_AT'], c = val_demo_bars_female['Label'],cmap=\"bwr\", marker = 'x', s = 2*val_demo_bars_female['Age'])\n",
    "\n",
    "#for vv in val_demo_bars_male[\"ID_ranked\"].unique():\n",
    "#    if val_demo_bars_male[\"ID_ranked\"].value_counts().loc[vv] > 1:\n",
    "#        val_demo_bars_male_lines = val_demo_bars_male[val_demo_bars_male[\"ID_ranked\"] == vv]\n",
    "#        ax.plot(val_demo_bars_male_lines['Bars'], val_demo_bars_male_lines['Prob_AT'],color =\"k\")\n",
    "\n",
    "#for vv in val_demo_bars_female[\"ID_ranked\"].unique():\n",
    "#    if val_demo_bars_female[\"ID_ranked\"].value_counts().loc[vv] > 1:\n",
    "#        val_demo_bars_female_lines = val_demo_bars_female[val_demo_bars_female[\"ID_ranked\"] == vv]\n",
    "#        ax.plot(val_demo_bars_female_lines['Bars'], val_demo_bars_female_lines['Prob_AT'],color =\"k\",linestyle =\"-\")\n",
    "\n",
    "plt.axhline(y=0.6, color='k', linestyle='--')\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "plt.xlim([-0.2, 30])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "ax.set_aspect(22.5)\n",
    "plt.xlabel(r'$\\rm{BARS_{total}}$', fontsize=40)\n",
    "plt.ylabel(r'$\\rm{P(Ataxia)}$', fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3727b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars = val_demo_all[['Bars','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars']= 0. \n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Sex'].notna()]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"].rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"].rank()-1\n",
    "    \n",
    "val_demo_bars_male = val_demo_bars_male[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).median()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "val_demo_bars_male_np_control = val_demo_bars_male[(val_demo_bars_male['Label'] == 0) ]['Prob_AT'].to_numpy()\n",
    "val_demo_bars_female_np_control = val_demo_bars_female[(val_demo_bars_female['Label'] == 0)]['Prob_AT'].to_numpy()\n",
    "\n",
    "val_demo_bars_male_np_ataxia = val_demo_bars_male[(val_demo_bars_male['Label'] == 1) ]['Prob_AT'].to_numpy()\n",
    "val_demo_bars_female_np_ataxia = val_demo_bars_female[(val_demo_bars_female['Label'] == 1)]['Prob_AT'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 sample test between male and female controls\n",
    "stats.ttest_ind(a=val_demo_bars_male_np_control, b=val_demo_bars_female_np_control, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aae624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 sample test between males and females withs AT\n",
    "stats.ttest_ind(a=val_demo_bars_male_np_ataxia, b=val_demo_bars_female_np_ataxia, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean P_AT for Males: ', np.mean(val_demo_bars_male_np_ataxia))\n",
    "print('Mean P_AT for Females: ', np.mean(val_demo_bars_female_np_ataxia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars = val_demo_all[['Bars','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars']= 0.\n",
    "val_demo_bars = val_demo_bars[val_demo_bars.Label == 1]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Age'].notna()]\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'M'),'Sex']= 0\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'F'),'Sex']= 1\n",
    "val_demo_bars[\"ID_ranked\"] = val_demo_bars[\"P_ID\"].rank()-1\n",
    "val_demo_bars = val_demo_bars.astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corellation between Age and P_AT\n",
    "stats.spearmanr(val_demo_bars['Age'], val_demo_bars['Prob_AT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0507fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars = val_demo_all[['Bars','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars']= 0.\n",
    "val_demo_bars = val_demo_bars[val_demo_bars.Label == 1]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars'].notna()]\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'M'),'Sex']= 0\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'F'),'Sex']= 1\n",
    "val_demo_bars[\"ID_ranked\"] = val_demo_bars[\"P_ID\"].rank()-1\n",
    "val_demo_bars = val_demo_bars.astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "val_demo_bars_15_AT = val_demo_bars[(val_demo_bars['Bars']>=0) & (val_demo_bars['Bars']<15) & (val_demo_bars['Label']==1)]\n",
    "val_demo_bars_30_AT = val_demo_bars[(val_demo_bars['Bars']>=15) & (val_demo_bars['Bars']<30) & (val_demo_bars['Label']==1)]\n",
    "\n",
    "val_demo_bars_15_AT_np = val_demo_bars_15_AT['Prob_AT'].to_numpy()\n",
    "val_demo_bars_30_AT_np = val_demo_bars_30_AT['Prob_AT'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d94b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two sample t-test between subjects with BARS<15 and BARS >15\n",
    "stats.ttest_ind(a=val_demo_bars_15_AT_np, b=val_demo_bars_30_AT_np, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72156191",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean P_AT for BARS < 15: ', np.mean(val_demo_bars_15_AT_np))\n",
    "print('Mean P_AT for BARS > 15: ', np.mean(val_demo_bars_30_AT_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08553926",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['Bars','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars']= 0. \n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars'].notna()]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"].rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"].rank()-1\n",
    "    \n",
    "val_demo_bars_male = val_demo_bars_male[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).median()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "\n",
    "upper_quantile_male = val_demo_bars_male_upper68['Prob_AT'] - val_demo_bars_male['Prob_AT']\n",
    "lower_quantile_male = val_demo_bars_male['Prob_AT'] - val_demo_bars_male_lower68['Prob_AT']\n",
    "\n",
    "upper_quantile_female = val_demo_bars_female_upper68['Prob_AT'] - val_demo_bars_female['Prob_AT']\n",
    "lower_quantile_female = val_demo_bars_female['Prob_AT'] - val_demo_bars_female_lower68['Prob_AT']\n",
    "\n",
    "male_month_diff = []\n",
    "male_prob_diff = []\n",
    "male_init_bars = []\n",
    "female_month_diff = []\n",
    "female_prob_diff = []\n",
    "female_init_bars = []\n",
    "\n",
    "#print(val_demo_bars_male['Date'].astype(str).str[5:9].astype(float)*12. + val_demo_bars_male['Date'].astype(str).str[9:11].astype(float))\n",
    "for vv in val_demo_bars_male['ID_ranked'].unique():\n",
    "    if val_demo_bars_male['ID_ranked'].value_counts().loc[vv] > 1:\n",
    "        val_demo_bars_male_lines = val_demo_bars_male[val_demo_bars_male['ID_ranked'] == vv]\n",
    "        val_demo_bars_male_lines = val_demo_bars_male_lines.sort_values(by=['Date'])\n",
    "        val_demo_bars_male_lines['Months'] = val_demo_bars_male_lines['Date'].astype(str).str[5:9].astype(float)*12. + val_demo_bars_male_lines['Date'].astype(str).str[9:11].astype(float)\n",
    "        month_diff = val_demo_bars_male_lines['Months'].iloc[-1]- val_demo_bars_male_lines['Months'].iloc[0]\n",
    "        if month_diff >= 2:\n",
    "            male_init_bars.append(val_demo_bars_male_lines['Bars'].iloc[0])\n",
    "            male_month_diff.append(month_diff)\n",
    "            male_prob_diff.append(val_demo_bars_male_lines['Prob_AT'].iloc[-1]- val_demo_bars_male_lines['Prob_AT'].iloc[0])\n",
    "\n",
    "for vv in val_demo_bars_female['ID_ranked'].unique():\n",
    "    if val_demo_bars_female['ID_ranked'].value_counts().loc[vv] > 1:\n",
    "        val_demo_bars_female_lines = val_demo_bars_female[val_demo_bars_female['ID_ranked'] == vv]\n",
    "        val_demo_bars_female_lines = val_demo_bars_female_lines.sort_values(by=['Date'])\n",
    "        val_demo_bars_female_lines['Months'] = val_demo_bars_female_lines['Date'].astype(str).str[5:9].astype(float)*12. + val_demo_bars_female_lines['Date'].astype(str).str[9:11].astype(float)\n",
    "        month_diff = val_demo_bars_female_lines['Months'].iloc[-1]- val_demo_bars_female_lines['Months'].iloc[0]\n",
    "        if month_diff >= 2:\n",
    "            female_init_bars.append(val_demo_bars_female_lines['Bars'].iloc[0])\n",
    "            female_month_diff.append(month_diff)\n",
    "            female_prob_diff.append(val_demo_bars_female_lines['Prob_AT'].iloc[-1]- val_demo_bars_female_lines['Prob_AT'].iloc[0])\n",
    "\n",
    "ax.scatter(male_month_diff, male_prob_diff, c = 'red', marker = 'o',s = 70)#10*np.array(male_init_bars))\n",
    "ax.scatter(female_month_diff, female_prob_diff, c = 'red', marker = 'x', s =70)#10 *np.array(female_init_bars))\n",
    "\n",
    "print(np.mean(male_prob_diff + female_prob_diff), np.std(male_prob_diff + female_prob_diff))\n",
    "\n",
    "plt.axhline(y=0., color='k', linestyle='--')\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "#ax.set_aspect(8)\n",
    "#plt.xlim([-0.2, 2])\n",
    "plt.ylim([-0.5, 0.5])\n",
    "plt.xlabel(r'$\\rm{Months}$', fontsize=40)\n",
    "plt.ylabel(r'$\\rm{\\Delta P(Ataxia)}$', fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['Bars_Speech','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars_Speech']= 0. \n",
    "#val_demo_bars = val_demo_bars[(val_demo_bars['Age'] > 0) & (val_demo_bars['Age'] <= 20)]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars_Speech'].notna()]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"].rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"].rank()-1\n",
    "print(len(val_demo_bars[val_demo_bars['Label'] == 0]))\n",
    "val_demo_bars_male = val_demo_bars_male[['Bars_Speech', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['Bars_Speech', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).median()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "\n",
    "upper_quantile_male = val_demo_bars_male_upper68['Prob_AT'] - val_demo_bars_male['Prob_AT']\n",
    "lower_quantile_male = val_demo_bars_male['Prob_AT'] - val_demo_bars_male_lower68['Prob_AT']\n",
    "\n",
    "upper_quantile_female = val_demo_bars_female_upper68['Prob_AT'] - val_demo_bars_female['Prob_AT']\n",
    "lower_quantile_female = val_demo_bars_female['Prob_AT'] - val_demo_bars_female_lower68['Prob_AT']\n",
    "\n",
    "#plt.errorbar(val_demo_bars_male['Bars'], val_demo_bars_male['Prob_AT'], yerr=[lower_quantile_male.to_numpy(),upper_quantile_male.to_numpy()], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "#plt.errorbar(val_demo_bars_female['Bars'], val_demo_bars_female['Prob_AT'], yerr=[lower_quantile_female.to_numpy(),upper_quantile_female.to_numpy()], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "\n",
    "#plt.errorbar(val_demo_bars_male['Bars'], val_demo_bars_male['Prob_AT'], yerr = val_demo_bars_male_std['Prob_AT'], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "#plt.errorbar(val_demo_bars_female['Bars'], val_demo_bars_female['Prob_AT'], yerr = val_demo_bars_female_std['Prob_AT'], fmt='none',c ='gray', capsize =4, elinewidth =0.5)\n",
    "\n",
    "ax.scatter(val_demo_bars_male['Bars_Speech'], val_demo_bars_male['Prob_AT'], c = val_demo_bars_male['Label'],cmap=\"bwr\", marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "ax.scatter(val_demo_bars_female['Bars_Speech'], val_demo_bars_female['Prob_AT'], c = val_demo_bars_female['Label'],cmap=\"bwr\", marker = 'x', s = 2*val_demo_bars_female['Age'])\n",
    "\n",
    "#for vv in val_demo_bars_male[\"ID_ranked\"].unique():\n",
    "#    if val_demo_bars_male[\"ID_ranked\"].value_counts().loc[vv] > 1:\n",
    "#        val_demo_bars_male_lines = val_demo_bars_male[val_demo_bars_male[\"ID_ranked\"] == vv]\n",
    "#        ax.plot(val_demo_bars_male_lines['Bars_Speech'], val_demo_bars_male_lines['Prob_AT'],color =\"k\")\n",
    "\n",
    "#for vv in val_demo_bars_female[\"ID_ranked\"].unique():\n",
    "#    if val_demo_bars_female[\"ID_ranked\"].value_counts().loc[vv] > 1:\n",
    "#        val_demo_bars_female_lines = val_demo_bars_female[val_demo_bars_female[\"ID_ranked\"] == vv]\n",
    "#        ax.plot(val_demo_bars_female_lines['Bars_Speech'], val_demo_bars_female_lines['Prob_AT'],color =\"k\",linestyle =\"-\")\n",
    "\n",
    "plt.axhline(y=0.6, color='k', linestyle='--')\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "ax.set_aspect(3)\n",
    "plt.xlim([-0.2, 4])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel(r'$\\rm{BARS_{Speech}}$', fontsize=40)\n",
    "plt.ylabel(r'$\\rm{P(Ataxia)}$', fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc951db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars = val_demo_all[['Bars_Speech','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars_Speech']= 0. \n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars_Speech'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars_Speech'] >= 0]\n",
    "\n",
    "val_demo_bars[\"ID_ranked\"] = val_demo_bars[\"P_ID\"].rank()-1\n",
    "    \n",
    "val_demo_bars = val_demo_bars[['Bars_Speech', 'Prob_AT','ID_ranked','Label', 'Age', 'Date']]\n",
    "val_demo_bars = val_demo_bars.astype('float64')\n",
    "\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "val_demo_bars_0_controls = val_demo_bars[(val_demo_bars['Bars_Speech']==0) & (val_demo_bars['Label']==0)]\n",
    "val_demo_bars_0_AT = val_demo_bars[(val_demo_bars['Bars_Speech']==0) & (val_demo_bars['Label']==1)]\n",
    "\n",
    "val_demo_bars_0_controls_np = val_demo_bars_0_controls['Prob_AT'].to_numpy()\n",
    "val_demo_bars_0_AT_np = val_demo_bars_0_AT['Prob_AT'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(a=val_demo_bars_0_controls_np, b=val_demo_bars_0_AT_np, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410728a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean P_AT for controls with BARS_Speech = 0: ', np.mean(val_demo_bars_0_controls_np))\n",
    "print('Mean P_AT for AT patients with BARS_Speech = 0: ', np.mean(val_demo_bars_0_AT_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(val_demo_bars_0_controls_np,val_demo_bars_0_AT_np,alternative = 'less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean P_AT for controls with BARS_Speech = 0: ', np.median(val_demo_bars_0_controls_np))\n",
    "print('Mean P_AT for AT patients with BARS_Speech = 0: ', np.median(val_demo_bars_0_AT_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78892ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars = val_demo_all[['Bars_Speech','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars_Speech']= 0.\n",
    "#val_demo_bars = val_demo_bars[val_demo_bars.Label == 1]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars_Speech'].notna()]\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'M'),'Sex']= 0\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'F'),'Sex']= 1\n",
    "val_demo_bars[\"ID_ranked\"] = val_demo_bars[\"P_ID\"].rank()-1\n",
    "val_demo_bars = val_demo_bars.astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "val_demo_bars_2_AT = val_demo_bars[(val_demo_bars['Bars_Speech']>=0) & (val_demo_bars['Bars_Speech']<2) & (val_demo_bars['Label']==1)]\n",
    "val_demo_bars_4_AT = val_demo_bars[(val_demo_bars['Bars_Speech']>=2) & (val_demo_bars['Bars_Speech']<4) & (val_demo_bars['Label']==1)]\n",
    "\n",
    "val_demo_bars_2_AT_np = val_demo_bars_2_AT['Prob_AT'].to_numpy()\n",
    "val_demo_bars_4_AT_np = val_demo_bars_4_AT['Prob_AT'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two sample t-test between subjects with BARS_speech<2 and BARS_speech >2\n",
    "stats.ttest_ind(a=val_demo_bars_2_AT_np, b=val_demo_bars_4_AT_np, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(val_demo_bars_2_AT_np,val_demo_bars_4_AT_np, alternative = 'less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean P_AT for BARS_speech < 2: ', np.mean(val_demo_bars_2_AT_np))\n",
    "print('Mean P_AT for BARS_speech > 2: ', np.mean(val_demo_bars_4_AT_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572133f",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for val_demo in val_demo_:\n",
    "    print('v'+str(i))\n",
    "    val_demo_effect = val_demo[['P_ID','Bars', 'Prob_control', 'Prob_AT', 'Label','Date']]\n",
    "    val_demo_effect.loc[(val_demo_effect.Label == 0),'Bars']= 0.\n",
    "    val_demo_effect = val_demo_effect.astype('float64')\n",
    "    val_demo_effect_grouped = val_demo_effect.groupby(['Date'], as_index = False).mean()\n",
    "    #val_demo_effect_grouped['Prediction'] = np.where(val_demo_effect_grouped['Prob_AT'] > val_demo_effect_grouped['Prob_control'], 1,0)\n",
    "    val_demo_effect_grouped['Prediction'] = np.where(val_demo_effect_grouped['Prob_AT'] > 0.5, 1,0)\n",
    "    val_lables_array = val_demo_effect_grouped['Label'].to_numpy().astype(int)\n",
    "    val_pred_array = val_demo_effect_grouped['Prediction'].to_numpy().astype(int)\n",
    "    val_prop_at_array = val_demo_effect_grouped['Prob_AT'].to_numpy()\n",
    "    print('f1_weighted:',f1_score(val_lables_array, val_pred_array, average='weighted'))\n",
    "    #fpr, tpr, thresholds = roc_curve(val_lables_array, val_prop_at_array, pos_label=1)\n",
    "    print('AUC:',roc_auc_score(val_lables_array, val_prop_at_array, average = 'weighted'))#auc(fpr, tpr))\n",
    "    cm = confusion_matrix(val_lables_array, val_pred_array, normalize= 'true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot() \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01097bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "f1_scores =[]\n",
    "AUC =[]\n",
    "for val_demo in val_demo__:\n",
    "    #if i == 3:\n",
    "    #    i+=1\n",
    "    #    continue\n",
    "    val_demo_effect = val_demo[['P_ID','Bars_Speech', 'Prob_control', 'Prob_AT', 'Label', 'Age','Date']]\n",
    "    val_demo_effect.loc[(val_demo_effect.Label == 0),'Bars_Speech']= 0.\n",
    "    #val_demo_effect = val_demo_effect[(val_demo_effect['Bars_Speech'] == 0)]\n",
    "    #val_demo_effect = val_demo_effect[(val_demo_effect['Age'] > 0) & (val_demo_effect['Age'] <= 20)]\n",
    "    val_demo_effect = val_demo_effect.astype('float64')\n",
    "    val_demo_effect_grouped = val_demo_effect.groupby(['Date'], as_index = False).median()\n",
    "    val_demo_effect_grouped['Prediction'] = np.where(val_demo_effect_grouped['Prob_AT'] > 0.6, 1,0)\n",
    "    val_lables_array = val_demo_effect_grouped['Label'].to_numpy().astype(int)\n",
    "    \n",
    "    val_pred_array = val_demo_effect_grouped['Prediction'].to_numpy().astype(int)\n",
    "    val_prop_at_array = val_demo_effect_grouped['Prob_AT'].to_numpy()\n",
    "    f1_scores.append(f1_score(val_lables_array, val_pred_array, average='weighted'))\n",
    "    fpr, tpr, thresholds = roc_curve(val_lables_array, val_prop_at_array, pos_label=1)\n",
    "    AUC.append(auc(fpr, tpr))\n",
    "    \n",
    "    if i == 0:\n",
    "        val_lables_array_all = val_lables_array\n",
    "        val_pred_array_all = val_pred_array\n",
    "        val_prop_at_array_all = val_prop_at_array\n",
    "    else:\n",
    "        val_lables_array_all = np.concatenate((val_lables_array_all, val_lables_array))\n",
    "        val_pred_array_all = np.concatenate((val_pred_array_all, val_pred_array))\n",
    "        val_prop_at_array_all = np.concatenate((val_prop_at_array_all, val_prop_at_array))\n",
    "    \n",
    "    i+=1\n",
    "print('total')\n",
    "print('f1_weighted:',np.mean(f1_scores),'+-', np.std(f1_scores) )\n",
    "print('AUC:',np.mean(AUC),'+-', np.std(AUC))\n",
    "\n",
    "cm = confusion_matrix(val_lables_array_all, val_pred_array_all, normalize= 'true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "disp.plot(cmap='gray', xticks_rotation='horizontal', values_format= '.2f' , ax=ax)\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.rcParams.update({'axes.labelsize': 35})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(val_lables_array_all, val_prop_at_array_all, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f ± %0.2f' % (np.mean(AUC), np.std(AUC)), linewidth = 3)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--',linewidth = 3)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel('TPR')\n",
    "plt.xlabel('FPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('f1_weighted:',np.mean(f1_scores),'+-', np.std(f1_scores) )\n",
    "print('AUC:',np.mean(AUC),'+-', np.std(AUC))\n",
    "print('f1_weighted_median:',np.median(f1_scores), '+', np.quantile(f1_scores,0.841)-np.median(f1_scores), '-', -np.quantile(f1_scores,0.159)+np.median(f1_scores) )\n",
    "print('AUC_median:',np.median(AUC), '+', np.quantile(AUC,0.841)-np.median(AUC), '-', -np.quantile(AUC,0.159)+np.median(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(val_lables_array_all, val_pred_array_all, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_distribution_at_max = []\n",
    "dist_distribution_control_max = []\n",
    "dist_distribution_at_max_f = []\n",
    "dist_distribution_control_max_f = []\n",
    "freq_loc = np.zeros((20,100))\n",
    "IG_loc = np.zeros((20,100))\n",
    "freq_loc_n = np.zeros(20)\n",
    "\n",
    "for i in [3]:#range(0,5):\n",
    "    print(i)\n",
    "    test_dataset = all_data[i]\n",
    "    trained_model = models[i]\n",
    "    for i_test in range(3):\n",
    "        \n",
    "        X_s, y_label,date_,mel_spec = test_dataset[i_test]\n",
    "        X_s = X_s.double()\n",
    "        mel_spec = mel_spec.double()\n",
    "        #######\n",
    "        ## log mel PCA in DB\n",
    "        pca = PCA(n_components=3)\n",
    "        S_DB_sum = torch.sum(mel_spec[0],axis=1)\n",
    "        S_trans = mel_spec[0]\n",
    "        temp_std =S_trans.std(axis=0)\n",
    "        temp_std[np.where(temp_std == 0)] = 1 # this ensures that we don't divide by zero\n",
    "        S_trans = (S_trans -S_trans.mean(axis=0)) / temp_std\n",
    "        pca.fit(S_trans)\n",
    "        S_pca = pca.transform(S_trans)\n",
    "        # if the first PC is not positively correlated with onset envelope, let's flip it\n",
    "        if np.corrcoef(S_DB_sum,S_pca[:,0])[0,1] < 0 :\n",
    "            pca.components_[0,:] = -1*pca.components_[0,:]\n",
    "            S_pca = pca.transform(S_trans)\n",
    "        #######\n",
    "        input_ = X_s.unsqueeze(0)\n",
    "        mel_spec = mel_spec.unsqueeze(0)\n",
    "        # Defining baseline distribution of images\n",
    "        rand_img_dist = torch.cat([input_ * 0, input_ * 1])\n",
    "        output = trained_model(input_)\n",
    "\n",
    "        prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "        pred_label_idx.squeeze_()\n",
    "        \n",
    "        integrated_gradients = IntegratedGradients(trained_model)\n",
    "        noise_tunnel = NoiseTunnel(integrated_gradients)\n",
    "        attributions_ig_nt = noise_tunnel.attribute(input_, nt_type='smoothgrad_sq', target=pred_label_idx,stdevs=0.0001)\n",
    "    \n",
    "        shap_values = np.rot90(np.transpose(attributions_ig_nt.cpu().detach().numpy().squeeze(0), (1,2,0)))\n",
    "        shap_x = np.sum(np.rot90(np.transpose(attributions_ig_nt.cpu().detach().numpy().squeeze(0), (1,2,0))),axis =0)\n",
    "        shap_y = np.sum(np.rot90(np.transpose(attributions_ig_nt.cpu().detach().numpy().squeeze(0), (1,2,0))),axis =1)\n",
    "        peaks, _ =scipy.signal.find_peaks(-S_pca[:,0],threshold =.05, width = 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(peaks) < 2:# or len(peaks_) < 2:\n",
    "            continue\n",
    "        \n",
    "        #print(shap_values.shape)\n",
    "        #print(shap_x.shape)\n",
    "        \n",
    "        shap_x_std = (shap_x - np.min(shap_x)) / (np.max(shap_x) - np.min(shap_x))\n",
    "        shap_x_scaled = shap_x_std * 100\n",
    "        shap_y_std = (shap_y - np.min(shap_y)) / (np.max(shap_y) - np.min(shap_y))\n",
    "        shap_y_scaled = shap_y_std * 100\n",
    "        \n",
    "        shap_x_std_tf = np.abs(shap_x_scaled[:,1])# + shap_x_scaled[:,1])\n",
    "        shap_x_std_tf = (shap_x_std_tf - np.min(shap_x_std_tf)) / (np.max(shap_x_std_tf) - np.min(shap_x_std_tf))\n",
    "        shap_x_std_tf = shap_x_std_tf * 100\n",
    "        #peaks_shap_x_t, _ =scipy.signal.find_peaks(np.abs(shap_x_scaled[:,0]))\n",
    "        #peaks_shap_x_f, _ =scipy.signal.find_peaks(np.abs(shap_x_scaled[:,1]))\n",
    "        peaks_shap_x_tf, _ =scipy.signal.find_peaks(shap_x_std_tf)\n",
    "        \n",
    "        \n",
    "        shap_y_std_tf = np.abs(shap_y_scaled[:,1])# + shap_y_scaled[:,1])\n",
    "        shap_y_std_tf = (shap_y_std_tf - np.min(shap_y_std_tf)) / (np.max(shap_y_std_tf) - np.min(shap_y_std_tf))\n",
    "        shap_y_std_tf = shap_y_std_tf * 100\n",
    "        #print(shap_y_std_tf)\n",
    "        #print(shap_y_std_tf[::-1])\n",
    "        peaks_shap_y_tf, _ =scipy.signal.find_peaks(shap_y_std_tf[::-1])\n",
    "        \n",
    "        if i_test == 2:\n",
    "            print(output[0])\n",
    "            print(pred_label_idx,y_label)\n",
    "            fig_, ax_ = plt.subplots()\n",
    "            fig_.set_size_inches(18.5, 10.5)\n",
    "            ax_.imshow(shap_values[:,:,0],extent=[0, 100, 0, 100])\n",
    "            plt.show()\n",
    "            \n",
    "            fig_, ax_ = plt.subplots()\n",
    "            fig_.set_size_inches(18.5, 10.5)\n",
    "            ax_.imshow(np.rot90(np.transpose(input_.cpu().detach().numpy().squeeze(0), (1,2,0)))[:,:,1],extent=[0, 100, 0, 100])\n",
    "            #ax_.imshow(np.rot90(np.transpose(mel_spec.cpu().detach().numpy().squeeze(0), (1,2,0))),extent=[0, 100, 0, 100])\n",
    "            x = range(100)\n",
    "            y = range(100)\n",
    "            #ax_.plot(np.abs(shap_y_std_tf)[::-1],y,c = 'tab:red',linewidth = 3)\n",
    "            #ax_.plot(x,np.abs(shap_x_scaled[:,0]),c = 'k',linewidth = 3)\n",
    "            #ax_.plot(x,np.abs(shap_x_scaled[:,1]),c = 'ivory',linewidth = 3)\n",
    "            ax_.plot(x,shap_x_std_tf,c = 'tab:red',linewidth = 5)\n",
    "            ax_.plot(x,S_pca[:,0] + 50, c = 'ivory',linewidth = 5)\n",
    "            #ax_.plot(peaks, S_pca[peaks,0]+50, \"x\", c ='k')\n",
    "            #ax_.plot(peaks_, S_pca[peaks_,0]+100, \"x\", c ='r')\n",
    "            #ax_.plot(peaks_shap_x_t, shap_x_scaled[peaks_shap_x_t,0], \"x\", c ='r')\n",
    "            #ax_.plot(peaks_shap_x_f, shap_x_scaled[peaks_shap_x_f,1], \"x\", c ='r')\n",
    "            #ax_.plot(peaks_shap_x_tf, shap_x_std_tf[peaks_shap_x_tf], \"x\", c ='k')\n",
    "            #ax_.plot( shap_y_std_tf[::-1][peaks_shap_y_tf], peaks_shap_y_tf,\"x\", c ='k')\n",
    "            #ax_.vlines(peaks, ymin =0 ,ymax = S_pca[peaks,0]+50, colors ='k')\n",
    "            ax_.vlines(peaks, ymin =0 ,ymax = 100, colors ='k')\n",
    "            ax_.set_aspect(0.66)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            fig_, ax_ = plt.subplots()\n",
    "            fig_.set_size_inches(18.5, 10.5)\n",
    "            ax_.imshow(np.rot90(np.transpose(input_.cpu().detach().numpy().squeeze(0), (1,2,0)))[:,:,0],extent=[0, 100, 0, 100])\n",
    "            #ax_.imshow(np.rot90(np.transpose(mel_spec.cpu().detach().numpy().squeeze(0), (1,2,0))),extent=[0, 100, 0, 100])\n",
    "            x = range(100)\n",
    "            y = range(100)\n",
    "            ax_.plot(np.abs(shap_y_std_tf)[::-1],y,c = 'orange',linewidth = 3)\n",
    "            #ax_.plot(x,np.abs(shap_x_scaled[:,0]),c = 'k',linewidth = 3)\n",
    "            #ax_.plot(x,np.abs(shap_x_scaled[:,1]),c = 'ivory',linewidth = 3)\n",
    "            #ax_.plot(x,shap_x_std_tf,c = 'orange',linewidth = 3)\n",
    "            #ax_.plot(x,S_pca[:,0] + 50, c = 'ivory',linewidth = 3)\n",
    "            #ax_.plot(peaks, S_pca[peaks,0]+50, \"x\", c ='k')\n",
    "            #ax_.plot(peaks_, S_pca[peaks_,0]+100, \"x\", c ='r')\n",
    "            #ax_.plot(peaks_shap_x_t, shap_x_scaled[peaks_shap_x_t,0], \"x\", c ='r')\n",
    "            #ax_.plot(peaks_shap_x_f, shap_x_scaled[peaks_shap_x_f,1], \"x\", c ='r')\n",
    "            #ax_.plot(peaks_shap_x_tf, shap_x_std_tf[peaks_shap_x_tf], \"x\", c ='orange')\n",
    "            ax_.plot( shap_y_std_tf[::-1][peaks_shap_y_tf], peaks_shap_y_tf,\"x\", c ='k')\n",
    "            #ax_.vlines(peaks, ymin =0 ,ymax = S_pca[peaks,0]+50, colors ='k')\n",
    "            #ax_.vlines(peaks, ymin =0 ,ymax = 100, colors ='k')\n",
    "            ax_.set_aspect(0.66)\n",
    "            plt.show()\n",
    "        \n",
    "        no_peaks = 1\n",
    "        for j in range(no_peaks):\n",
    "            if pred_label_idx !=  y_label[0]:\n",
    "                continue\n",
    "            \n",
    "            IG_peaks = np.argpartition(-shap_x_std_tf[peaks_shap_x_tf], no_peaks)[j]\n",
    "            diff = peaks - peaks_shap_x_tf[IG_peaks]\n",
    "            diff_min = np.argmin(np.abs(diff))\n",
    "            #print(peaks_shap_x[IG_peaks],diff, period)\n",
    "            if diff[diff_min] <= 0:\n",
    "                if (diff_min + 1) < len(peaks):\n",
    "                    period = peaks[diff_min + 1] - peaks[diff_min]\n",
    "                else:\n",
    "                    period = peaks[diff_min] - peaks[diff_min-1]\n",
    "                    \n",
    "                location = -diff[diff_min]/period\n",
    "            else:\n",
    "                if (diff_min - 1) >= 0:\n",
    "                    period = peaks[diff_min] - peaks[diff_min-1]\n",
    "                else:\n",
    "                    period = peaks[diff_min + 1] - peaks[diff_min]\n",
    "                \n",
    "                location = (period - diff[diff_min])/period\n",
    "                \n",
    "            location = location % 1\n",
    "            \n",
    "            if y_label[0] == 0:\n",
    "                dist_distribution_control_max.append(location)  \n",
    "            else:\n",
    "                dist_distribution_at_max.append(location) \n",
    "                \n",
    "            IG_peaks_f = np.argpartition(-shap_y_std_tf[::-1][peaks_shap_y_tf], no_peaks)[j]\n",
    "            \n",
    "            if y_label[0] == 0:\n",
    "                dist_distribution_control_max_f.append(peaks_shap_y_tf[IG_peaks_f])  \n",
    "            else:\n",
    "                dist_distribution_at_max_f.append(peaks_shap_y_tf[IG_peaks_f]) \n",
    "\n",
    "            #if np.abs(location) <=1:\n",
    "            #    spectrum_sample = np.rot90(np.transpose(input_.cpu().detach().numpy().squeeze(0), (1,2,0)))\n",
    "            #    index = int(np.abs(location)//0.05)\n",
    "            #    freq_loc_n[index] += 1\n",
    "            #    freq_loc[index] = freq_loc[index] + spectrum_sample[:,peaks_shap_x_tf[IG_peaks],0]\n",
    "            #    IG_loc[index] = IG_loc[index] + shap_values[:,peaks_shap_x_tf[IG_peaks],0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cddad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = np.msort(np.abs(dist_distribution_at_max + dist_distribution_control_max))\n",
    "#test_data_ =  [(x % 1) for x in test_data_ if x > 1] + [x for x in test_data_ if x<=1]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "hist = plt.hist(test_data_, bins=20, density=False, alpha=0.5,\n",
    "         histtype='stepfilled', color='steelblue',\n",
    "         edgecolor='none');\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "plt.ylabel(\"Counts\", fontsize=40)\n",
    "plt.xlabel(\"Δt/T\", fontsize=40)\n",
    "#plt.vlines([0.5], ymin =0 ,ymax = 600, color = 'k')\n",
    "#ax.set_aspect(0.0015)\n",
    "plt.xlim([0.0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65527b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = np.msort(np.abs(dist_distribution_at_max))\n",
    "#test_data_ =  [(x % 1) for x in test_data_ if x > 1] + [x for x in test_data_ if x<=1]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "hist = plt.hist(test_data_, bins=20, density=False, alpha=0.5,\n",
    "         histtype='stepfilled', color='steelblue',\n",
    "         edgecolor='none');\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "plt.ylabel(\"Counts\", fontsize=40)\n",
    "plt.xlabel(\"Δt/T\", fontsize=40)\n",
    "#plt.vlines([0.5], ymin =0 ,ymax = 600, color = 'k')\n",
    "#ax.set_aspect(0.0015)\n",
    "plt.xlim([0.0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7af554",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = np.msort(np.abs(dist_distribution_control_max))\n",
    "#test_data_ =  [(x % 1) for x in test_data_ if x > 1] + [x for x in test_data_ if x<=1]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "hist = plt.hist(test_data_, bins=20, density=False, alpha=0.5,\n",
    "         histtype='stepfilled', color='steelblue',\n",
    "         edgecolor='none');\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "plt.ylabel(\"Counts\", fontsize=40)\n",
    "plt.xlabel(\"Δt/T\", fontsize=40)\n",
    "#plt.vlines([0.5], ymin =0 ,ymax = 600, color = 'k')\n",
    "#ax.set_aspect(0.0015)\n",
    "plt.xlim([0.0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniformity test\n",
    "test_data = np.msort(test_data_)\n",
    "stat, p = kstest(test_data, uniform(loc=0, scale=1).cdf)#shapiro(test_data)\n",
    "print('Statistics=%.3f, p=%.45f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Sample looks Uniform (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Sample does not look Uniform (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df483d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    freq_loc[i] = freq_loc[i] / freq_loc_n[i]\n",
    "    IG_loc[i] = IG_loc[i] / freq_loc_n[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_, ax_ = plt.subplots()\n",
    "fig_.set_size_inches(10, 10)\n",
    "ax_.imshow(freq_loc.T,extent=[0, 1, 0, 128])\n",
    "plt.vlines([0.5], ymin =0 ,ymax = 128, color = 'k')\n",
    "ax_.set_aspect(0.0065)\n",
    "ax_.tick_params(axis='x', labelsize=35)\n",
    "ax_.tick_params(axis='y', labelsize=35)\n",
    "#ax.set_aspect(0.333)\n",
    "plt.xlabel(\"Δt/T\", fontsize=40)\n",
    "plt.ylabel(\"frequency\", fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_, ax_ = plt.subplots()\n",
    "fig_.set_size_inches(10, 10)\n",
    "ax_.imshow(IG_loc.T,extent=[0, 1, 0, 128])\n",
    "plt.vlines([0.5], ymin =0 ,ymax = 128, color = 'k')\n",
    "ax_.set_aspect(0.0065)\n",
    "ax_.tick_params(axis='x', labelsize=35)\n",
    "ax_.tick_params(axis='y', labelsize=35)\n",
    "#ax.set_aspect(0.333)\n",
    "plt.xlabel(\"Δt/T\", fontsize=40)\n",
    "plt.ylabel(\"frequency\", fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddabd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_effect = val_demo_all[['P_ID','Bars_Speech', 'Prob_control', 'Prob_AT', 'Label', 'Age','Date']]\n",
    "val_demo_effect.loc[(val_demo_effect.Label == 0),'Bars_Speech']= 0.\n",
    "val_demo_effect = val_demo_effect.astype('float64')\n",
    "val_demo_effect_grouped = val_demo_effect.groupby(['Date'], as_index = False).median()\n",
    "val_demo_effect_grouped = val_demo_effect_grouped[val_demo_effect_grouped['Bars_Speech'] == 0]\n",
    "val_demo_effect_grouped['Prediction'] = np.where(val_demo_effect_grouped['Prob_AT'] > 0.6, 1,0)\n",
    "val_lables_array = val_demo_effect_grouped['Label'].to_numpy().astype(int)\n",
    "val_pred_array = val_demo_effect_grouped['Prediction'].to_numpy().astype(int)\n",
    "val_prop_at_array = val_demo_effect_grouped['Prob_AT'].to_numpy()\n",
    "print(f1_score(val_lables_array, val_pred_array, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89532d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ac103",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = np.msort(np.abs(dist_distribution_at_max_f+ dist_distribution_control_max_f))\n",
    "#test_data_ =  [(x % 1) for x in test_data_ if x > 1] + [x for x in test_data_ if x<=1]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "hist = plt.hist(test_data_, bins=20, density=False, alpha=0.5,\n",
    "         histtype='stepfilled', color='steelblue',\n",
    "         edgecolor='none');\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "plt.ylabel(\"Counts\", fontsize=40)\n",
    "plt.xlabel(\"frequency\", fontsize=40)\n",
    "#plt.vlines([0.5], ymin =0 ,ymax = 600, color = 'k')\n",
    "#ax.set_aspect(0.0015)\n",
    "plt.xlim([0.0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b527777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = np.msort(np.abs(dist_distribution_at_max_f))\n",
    "#test_data_ =  [(x % 1) for x in test_data_ if x > 1] + [x for x in test_data_ if x<=1]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "hist = plt.hist(test_data_, bins=20, density=False, alpha=0.5,\n",
    "         histtype='stepfilled', color='steelblue',\n",
    "         edgecolor='none');\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "plt.ylabel(\"Counts\", fontsize=40)\n",
    "plt.xlabel(\"frequency\", fontsize=40)\n",
    "#plt.vlines([0.5], ymin =0 ,ymax = 600, color = 'k')\n",
    "#ax.set_aspect(0.0015)\n",
    "plt.xlim([0.0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada31b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = np.msort(np.abs(dist_distribution_control_max_f))\n",
    "#test_data_ =  [(x % 1) for x in test_data_ if x > 1] + [x for x in test_data_ if x<=1]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "hist = plt.hist(test_data_, bins=20, density=False, alpha=0.5,\n",
    "         histtype='stepfilled', color='steelblue',\n",
    "         edgecolor='none');\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "plt.ylabel(\"Counts\", fontsize=40)\n",
    "plt.xlabel(\"frequency\", fontsize=40)\n",
    "#plt.vlines([0.5], ymin =0 ,ymax = 600, color = 'k')\n",
    "#ax.set_aspect(0.0015)\n",
    "plt.xlim([0.0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_distribution_at_max_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b9273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df001b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2, sharey = ax1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['Bars','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars']= 0. \n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars'].notna()]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"].rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"].rank()-1\n",
    "    \n",
    "val_demo_bars_male = val_demo_bars_male[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['Bars', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).mean()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "\n",
    "upper_quantile_male = val_demo_bars_male_upper68['Prob_AT'] - val_demo_bars_male['Prob_AT']\n",
    "lower_quantile_male = val_demo_bars_male['Prob_AT'] - val_demo_bars_male_lower68['Prob_AT']\n",
    "\n",
    "upper_quantile_female = val_demo_bars_female_upper68['Prob_AT'] - val_demo_bars_female['Prob_AT']\n",
    "lower_quantile_female = val_demo_bars_female['Prob_AT'] - val_demo_bars_female_lower68['Prob_AT']\n",
    "\n",
    "ax1.scatter(val_demo_bars_male['Bars'], val_demo_bars_male['Prob_AT'], c = val_demo_bars_male['Label'],cmap=\"bwr\", marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "ax1.scatter(val_demo_bars_female['Bars'], val_demo_bars_female['Prob_AT'], c = val_demo_bars_female['Label'],cmap=\"bwr\", marker = 'x', s = 2*val_demo_bars_female['Age'])\n",
    "\n",
    "ax1.axhline(y=0.5, color='k', linestyle='--')\n",
    "\n",
    "\n",
    "val_demo_bars = val_demo_all[['Bars_Speech','Prob_AT','P_ID','Sex','Label', 'Age','Date']]\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars_Speech']= 0. \n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars_Speech'].notna()]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"].rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"].rank()-1\n",
    "    \n",
    "val_demo_bars_male = val_demo_bars_male[['Bars_Speech', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['Bars_Speech', 'Prob_AT','ID_ranked','Label', 'Age','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).mean()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "\n",
    "upper_quantile_male = val_demo_bars_male_upper68['Prob_AT'] - val_demo_bars_male['Prob_AT']\n",
    "lower_quantile_male = val_demo_bars_male['Prob_AT'] - val_demo_bars_male_lower68['Prob_AT']\n",
    "\n",
    "upper_quantile_female = val_demo_bars_female_upper68['Prob_AT'] - val_demo_bars_female['Prob_AT']\n",
    "lower_quantile_female = val_demo_bars_female['Prob_AT'] - val_demo_bars_female_lower68['Prob_AT']\n",
    "\n",
    "ax2.scatter(val_demo_bars_male['Bars_Speech'], val_demo_bars_male['Prob_AT'], c = val_demo_bars_male['Label'],cmap=\"bwr\", marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "ax2.scatter(val_demo_bars_female['Bars_Speech'], val_demo_bars_female['Prob_AT'], c = val_demo_bars_female['Label'],cmap=\"bwr\", marker = 'x', s = 2*val_demo_bars_female['Age'])\n",
    "\n",
    "ax2.axhline(y=0.5, color='k', linestyle='--')\n",
    "\n",
    "ax1.tick_params(axis='x', labelsize=35)\n",
    "ax1.tick_params(axis='y', labelsize=35)\n",
    "ax2.tick_params(axis='x', labelsize=35)\n",
    "ax2.tick_params(axis='y', labelsize=0)\n",
    "\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params('both', length=10, width=2, which='major',direction=\"in\")\n",
    "ax1.tick_params('both', length=5, width=1, which='minor',direction=\"in\")\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params('both', length=10, width=2, which='major',direction=\"in\")\n",
    "ax2.tick_params('both', length=5, width=1, which='minor',direction=\"in\")\n",
    "\n",
    "ax1.set_aspect(20)\n",
    "ax1.set_xlim([-1, 31])\n",
    "ax1.set_ylim([-0.05, 1.03])\n",
    "\n",
    "ax2.set_aspect(2.63)\n",
    "ax2.set_xlim([-0.2, 4])\n",
    "ax2.set_ylim([-0.05, 1.03])\n",
    "\n",
    "ax1.set_xlabel(r'$\\rm{BARS_{total}}$', fontsize=40)\n",
    "ax1.set_ylabel(r'$\\rm{P(Ataxia)}$', fontsize=40)\n",
    "ax2.set_xlabel(r'$\\rm{BARS_{speech}}$', fontsize=40)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49877fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters_single_channel_big(t):\n",
    "    \n",
    "    #setting the rows and columns\n",
    "    nrows = t.shape[0]*t.shape[2]\n",
    "    ncols = t.shape[1]*t.shape[3]\n",
    "    \n",
    "    \n",
    "    npimg = np.array(t.numpy(), np.float32)\n",
    "    npimg = npimg.transpose((0, 2, 1, 3))\n",
    "    npimg = npimg.ravel().reshape(nrows, ncols)\n",
    "    \n",
    "    npimg = npimg.T\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(ncols/10, nrows/200))    \n",
    "    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters_single_channel(t):\n",
    "    \n",
    "    #kernels depth * number of kernels\n",
    "    nplots = t.shape[0]*t.shape[1]\n",
    "    ncols = 12\n",
    "    \n",
    "    nrows = 1 + nplots//ncols\n",
    "    #convert tensor to numpy image\n",
    "    npimg = np.array(t.numpy(), np.float32)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    #looping through all the kernels in each channel\n",
    "    for i in range(t.shape[0]):\n",
    "        for j in range(t.shape[1]):\n",
    "            fig = plt.figure()\n",
    "            count += 1\n",
    "            ax1 = fig.add_subplot(1, 1, 1)\n",
    "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
    "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "            ax1.imshow(npimg)\n",
    "            ax1.set_title(str(i) + ',' + str(j))\n",
    "            ax1.axis('off')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters_multi_channel(t):\n",
    "    \n",
    "    #get the number of kernals\n",
    "    num_kernels = t.shape[0]    \n",
    "    \n",
    "    #define number of columns for subplots\n",
    "    num_cols = 12\n",
    "    #rows = num of kernels\n",
    "    num_rows = num_kernels\n",
    "    \n",
    "    #set the figure size\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    \n",
    "    #looping through all the kernels\n",
    "    for i in range(t.shape[0]):\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "        \n",
    "        #for each kernel, we convert the tensor to numpy \n",
    "        npimg = np.array(t[i].numpy(), np.float32)\n",
    "        #standardize the numpy image\n",
    "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "        npimg = npimg.transpose((1, 2, 0))\n",
    "        ax1.imshow(npimg)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(str(i))\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        \n",
    "        #plt.savefig('myimage.png', dpi=100)    \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(model, single_channel = True, collated = False):\n",
    "    \n",
    "    #getting the weight tensor data\n",
    "    weight_tensor = model.model.C1.weight.data\n",
    "\n",
    "    if single_channel:\n",
    "        if collated:\n",
    "            plot_filters_single_channel_big(weight_tensor)\n",
    "        else:\n",
    "            plot_filters_single_channel(weight_tensor)\n",
    "\n",
    "    else:\n",
    "        if weight_tensor.shape[1] == 3:\n",
    "            plot_filters_multi_channel(weight_tensor)\n",
    "        else:\n",
    "            print(\"Can only plot weights with three channels with single channel = False\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d80dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize weights for alexnet - first conv layer\n",
    "plot_weights(trained_model_v0,  single_channel = True, collated = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_v0.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e070a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_v0.model.C1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36771862",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(trained_model_v0.model.C1.weight.data.numpy()[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = trained_model_v0.model.C1.weight.data\n",
    "for i in range(kernels.shape[0]):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1, 1, 1)\n",
    "    ax1.imshow((kernels.numpy()[i][0] - np.mean(kernels.numpy()[i][0]))/np.std(kernels.numpy()[i][0]))\n",
    "    ax1.axis('off')\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5d888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
