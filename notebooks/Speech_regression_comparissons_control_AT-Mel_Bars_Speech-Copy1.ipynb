{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6581a5",
   "metadata": {},
   "source": [
    "# Speech classifier for NDs using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro,normaltest,kstest,uniform\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "sys.path.append('../../')\n",
    "\n",
    "#sklearn \n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score, roc_curve,auc, roc_auc_score,ConfusionMatrixDisplay\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch. optim.lr_scheduler import _LRScheduler\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "#Pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from pytorch_lightning import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "#models\n",
    "from script.models import FC_Resnet_comp_\n",
    "\n",
    "#utils\n",
    "from script.utils import PadImage_inf_comp, PadImage_inf_comp_val\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "#Captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857570b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(42)\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45923f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definition\n",
    "epochs = 100 # no of epochs\n",
    "Batch_Size = 128 #batch size\n",
    "no_feutures = 128 #no of features per entry\n",
    "training_on = False\n",
    "fold = 0\n",
    "root_dir = '/home/kvattis/Documents/data/'\n",
    "train_csv_file = root_dir + 'train_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v' + str(fold) + '.csv'\n",
    "val_csv_file = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v' + str(fold)+ '.csv'\n",
    "train_demo_csv_file = root_dir +'train_demo_Mel_severity_cnn_nr2_v' + str(fold)+ '.csv'\n",
    "val_demo_csv_file = root_dir + 'val_demo_Mel_severity_cnn_nr2_v' + str(fold)+ '.csv'\n",
    "parent_directory = '/home/kvattis/Documents/speech_analysis/'\n",
    "checkpoint_directory = parent_directory + 'checkpoints/resnet_regression_speech_10fold_fresh_comparisons_pearsons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff64e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = [3487,8145]\n",
    "weights = [1/x for x in n_class]\n",
    "weights = [ww/np.sum(weights) for ww in weights]\n",
    "#weights = [0.75, 0.25]\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(X, range_=(0, 1)):\n",
    "    mi, ma = range_\n",
    "    X_min = -50\n",
    "    X_max = 50\n",
    "    #X_std = (X - X.min()) / (X.max() - X.min())                                                                                                                                                              \n",
    "    X_std = (X - X_min) / (X_max - X_min)\n",
    "    X_scaled = X_std * (ma - mi) + mi\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms_train(spec,l):\n",
    "    upper_limit = spec.shape[1]\n",
    "    if l == 0:\n",
    "        random_size = random.randint(20,upper_limit)\n",
    "        transforms_ = transforms.Compose([transforms.RandomCrop((random_size, 128)),transforms.Resize((100, 100))])\n",
    "    else:\n",
    "        random_size = random.randint(20,upper_limit)\n",
    "        transforms_ = transforms.Compose([transforms.RandomCrop((random_size, 128)),transforms.Resize((100, 100))])\n",
    "    spec = transforms_(spec)\n",
    "    return spec\n",
    "\n",
    "def transforms_val(spec,l):\n",
    "    transforms_resize = transforms.Resize((100, 100))\n",
    "    spec = transforms_resize(spec)\n",
    "    return spec\n",
    "\n",
    "def global_std(X, mean = -0.0005, std = 0.0454):\n",
    "    X_scaled = (X - mean)/ std\n",
    "    return X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    \n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f760854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain(spec):\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711002a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_mean(value:torch.Tensor, labels:torch.LongTensor) -> (torch.Tensor, torch.LongTensor):\n",
    "    \"\"\"Group-wise average for (sparse) grouped tensors\n",
    "\n",
    "    Args:\n",
    "        value (torch.Tensor): values to average (# samples, latent dimension)\n",
    "        labels (torch.LongTensor): labels for embedding parameters (# samples,)\n",
    "\n",
    "    Returns: \n",
    "        result (torch.Tensor): (# unique labels, latent dimension)\n",
    "        new_labels (torch.LongTensor): (# unique labels,)\n",
    "\n",
    "    Examples:\n",
    "        >>> samples = torch.Tensor([\n",
    "                             [0.15, 0.15, 0.15],    #-> group / class 1\n",
    "                             [0.2, 0.2, 0.2],    #-> group / class 3\n",
    "                             [0.4, 0.4, 0.4],    #-> group / class 3\n",
    "                             [0.0, 0.0, 0.0]     #-> group / class 0\n",
    "                      ])\n",
    "        >>> labels = torch.LongTensor([1, 5, 5, 0])\n",
    "        >>> result, new_labels = groupby_mean(samples, labels)\n",
    "\n",
    "        >>> result\n",
    "        tensor([[0.0000, 0.0000, 0.0000],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.3000, 0.3000, 0.3000]])\n",
    "\n",
    "        >>> new_labels\n",
    "        tensor([0, 1, 5])\n",
    "    \"\"\"\n",
    "    uniques = labels.unique().tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    key_val = {key: val for key, val in zip(uniques, range(len(uniques)))}\n",
    "    val_key = {val: key for key, val in zip(uniques, range(len(uniques)))}\n",
    "\n",
    "    labels = torch.LongTensor(list(map(key_val.get, labels)))\n",
    "\n",
    "    labels = labels.view(labels.size(0), 1).expand(-1, value.size(1))\n",
    "\n",
    "    unique_labels, labels_count = labels.unique(dim=0, return_counts=True)\n",
    "    result = torch.zeros_like(unique_labels.to(device), dtype=value.dtype).scatter_add_(0, labels.to(device), value.to(device))\n",
    "    result = result.to(device) / labels_count.float().unsqueeze(1).to(device)\n",
    "    new_labels = torch.LongTensor(list(map(val_key.get, unique_labels[:, 0].tolist())))\n",
    "    return result.to(device), new_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a pytorch Dataset                                                                                                                                                                                     \n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, csv_file, demo_csv, root_dir,transform):\n",
    "\n",
    "        self.file_names = pd.read_csv(csv_file,header = None, names=[\"No\",\"P_ID\", \"Address\",\"Label\",\"Date\"])\n",
    "        self.demo = pd.read_csv(demo_csv, names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\",\"Bars_Speech\", \"PDate\"])\n",
    "        self.file_names['Bars'] = self.demo['Bars_Speech']\n",
    "        #self.file_names['Age'] = self.demo['Age']                                                                                                                                                            \n",
    "        #self.file_names = self.file_names[self.file_names['Age']<18]                                                                                                                                         \n",
    "        self.file_names.loc[(self.file_names.Label == 0),'Bars']= 0.\n",
    "        #self.file_names = self.file_names[self.file_names.Label == 1]                                                                                                                                        \n",
    "        self.file_names = self.file_names[self.file_names.Bars >= 0]\n",
    "        self.file_names_bars = self.file_names[self.file_names['Bars'].notna()]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names_bars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        idx_comp = random.randrange(len(self.file_names_bars))\n",
    "        bars_ = self.file_names_bars.iloc[idx, 5]\n",
    "        bars_comp_ = self.file_names_bars.iloc[idx_comp, 5]\n",
    "\n",
    "        while (idx_comp == idx) or (self.file_names_bars.iloc[idx, 1] == self.file_names_bars.iloc[idx_comp, 1]) or (np.abs(bars_ - bars_comp_) < 0.5):\n",
    "            idx_comp = random.randrange(len(self.file_names_bars))\n",
    "            bars_comp_ = self.file_names_bars.iloc[idx_comp, 5]\n",
    "\n",
    "\n",
    "        #############                                                                                                                                                                                         \n",
    "\n",
    "        address =  os.path.join(self.root_dir,\n",
    "                                self.file_names_bars.iloc[idx, 2])\n",
    "\n",
    "        df = pd.read_csv(address,header = None)\n",
    "        df_ar = df.to_numpy()\n",
    "        df_ar = min_max_scale(df_ar)\n",
    "\n",
    "        df_ar_t = np.gradient(df_ar, axis = 0)\n",
    "        #df_ar_t = butterFilter(df_ar_t, cutoff_frequency, nyq_freq/2., order = order)                                                                                                                        \n",
    "        #df_ar_t_p = np.where(df_ar_t > 0, df_ar_t, 0)                                                                                                                                                        \n",
    "        #df_ar_t_n = np.abs(np.where(df_ar_t < 0, df_ar_t, 0))                                                                                                                                                \n",
    "\n",
    "        df_ar_f = np.gradient(df_ar, axis = 1)\n",
    "        #df_ar_f = butterFilter(df_ar_f, cutoff_frequency, nyq_freq/2., order = order)                                                                                                                        \n",
    "        #df_ar_f_p = np.where(df_ar_f > 0, df_ar_f, 0)                                                                                                                                                        \n",
    "        #df_ar_f_n = np.abs(np.where(df_ar_f < 0, df_ar_f, 0))                                                                                                                                                \n",
    "\n",
    "        #df_ar = np.stack((df_ar_t_p,df_ar_t_n,df_ar_f_p,df_ar_f_n), axis=0)\n",
    "        df_ar = np.stack((df_ar_t,df_ar_f), axis=0)\n",
    "\n",
    "        #df_ar_t = global_std(df_ar_t)                                                                                                                                                                        \n",
    "        #data = torch.Tensor(df_ar_t.copy())                                                                                                                                                                  \n",
    "        df_ar = global_std(df_ar)\n",
    "        data = torch.DoubleTensor(df_ar.copy())\n",
    "\n",
    "        label_ = self.file_names_bars.iloc[idx, 3]\n",
    "        label = torch.LongTensor([label_])\n",
    "        p_id = self.file_names_bars.iloc[idx, 1]\n",
    "        adr_id = int(str(p_id) + str(self.file_names_bars.iloc[idx, 4]))\n",
    "        adr_id = torch.LongTensor([adr_id])\n",
    "        bars = self.file_names_bars.iloc[idx, 5]\n",
    "        #bars = torch.DoubleTensor([bars])                                                                                                                                                                    \n",
    "\n",
    "\n",
    "        #####################                                                                                                                                                                                 \n",
    "\n",
    "        address_comp =  os.path.join(self.root_dir,\n",
    "                                self.file_names_bars.iloc[idx_comp, 2])\n",
    "\n",
    "        df_comp = pd.read_csv(address_comp,header = None)\n",
    "        df_ar_comp = df_comp.to_numpy()\n",
    "        df_ar_comp = min_max_scale(df_ar_comp)\n",
    "\n",
    "        df_ar_t_comp = np.gradient(df_ar_comp, axis = 0)\n",
    "\n",
    "        df_ar_f_comp = np.gradient(df_ar_comp, axis = 1)\n",
    "\n",
    "        df_ar_comp = np.stack((df_ar_t_comp,df_ar_f_comp), axis=0)\n",
    "\n",
    "        #df_ar_t_comp = global_std(df_ar_t_comp)  \n",
    "        #data_comp = torch.Tensor(df_ar_t_comp.copy())                                                                                                                                                       \\\n",
    "                                                                                                                                                                                                              \n",
    "        df_ar_comp = global_std(df_ar_comp)\n",
    "        data_comp = torch.DoubleTensor(df_ar_comp.copy())\n",
    "\n",
    "        label__comp = self.file_names_bars.iloc[idx_comp, 3]\n",
    "        label_comp = torch.LongTensor([label__comp])\n",
    "        p_id_comp = self.file_names_bars.iloc[idx_comp, 1]\n",
    "        adr_id_comp = int(str(p_id_comp) + str(self.file_names_bars.iloc[idx_comp, 4]))\n",
    "        adr_id_comp = torch.LongTensor([adr_id_comp])\n",
    "        bars_comp = self.file_names_bars.iloc[idx_comp, 5]\n",
    "        #bars_comp = torch.DoubleTensor([bars_comp])                                                                                                                                                          \n",
    "\n",
    "        ############################                                                                                                                                                                          \n",
    "\n",
    "        if bars <= bars_comp:\n",
    "            output_label = 0\n",
    "        else:\n",
    "            output_label = 1\n",
    "\n",
    "\n",
    "        output_label = torch.LongTensor([output_label])\n",
    "\n",
    "\n",
    "        #data = torch.unsqueeze(data, 0)                                                                                                                                                                      \n",
    "        if self.transform:\n",
    "            data = self.transform(data,label_)#self.transform(data.T)                                                                                                                                         \n",
    "            #data = data.T                                                                                                                                                                                    \n",
    "\n",
    "            data_comp = self.transform(data_comp,label_)\n",
    "\n",
    "        return data, data_comp, output_label\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a pytorch Dataset                                                                                                                                                                                     \n",
    "class SpeechDataset_val(Dataset):\n",
    "    def __init__(self, csv_file, demo_csv, root_dir,transform):\n",
    "\n",
    "        self.file_names = pd.read_csv(csv_file,header = None, names=[\"No\",\"P_ID\", \"Address\",\"Label\",\"Date\"])\n",
    "        self.demo = pd.read_csv(demo_csv, names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\",\"Bars_Speech\", \"PDate\"])\n",
    "        self.file_names['Bars'] = self.demo['Bars_Speech']\n",
    "        #self.file_names['Age'] = self.demo['Age']                                                                                                                                                            \n",
    "        #self.file_names = self.file_names[self.file_names['Age']<18]                                                                                                                                         \n",
    "        self.file_names.loc[(self.file_names.Label == 0),'Bars']= 0.\n",
    "        #self.file_names = self.file_names[self.file_names.Label == 1]                                                                                                                                        \n",
    "        self.file_names = self.file_names[self.file_names.Bars >= 0]\n",
    "        self.file_names_bars = self.file_names[self.file_names['Bars'].notna()]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names_bars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        #############                                                                                                                                                                                         \n",
    "\n",
    "        address =  os.path.join(self.root_dir,\n",
    "                                self.file_names_bars.iloc[idx, 2])\n",
    "\n",
    "        df = pd.read_csv(address,header = None)\n",
    "        df_ar = df.to_numpy()\n",
    "        df_ar = min_max_scale(df_ar)\n",
    "\n",
    "        df_ar_t = np.gradient(df_ar, axis = 0)\n",
    "\n",
    "        df_ar_f = np.gradient(df_ar, axis = 1)\n",
    "\n",
    "        df_ar = np.stack((df_ar_t,df_ar_f), axis=0)\n",
    "\n",
    "        #df_ar_t = global_std(df_ar_t)                                                                                                                                                                        \n",
    "        #data = torch.Tensor(df_ar_t.copy())                                                                                                                                                                  \n",
    "        df_ar = global_std(df_ar)\n",
    "        data = torch.DoubleTensor(df_ar.copy())\n",
    "\n",
    "        label_ = self.file_names_bars.iloc[idx, 3]\n",
    "        label = torch.LongTensor([label_])\n",
    "        p_id = self.file_names_bars.iloc[idx, 1]\n",
    "        adr_id = int(str(p_id) + str(self.file_names_bars.iloc[idx, 4]))\n",
    "        adr_id = torch.LongTensor([adr_id])\n",
    "        bars = self.file_names_bars.iloc[idx, 5]\n",
    "        bars = torch.DoubleTensor([bars])\n",
    "\n",
    "\n",
    "        #data = torch.unsqueeze(data, 0)                                                                                                                                                                      \n",
    "        if self.transform:\n",
    "            data = self.transform(data,label_)\n",
    "\n",
    "        return data, bars, adr_id, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae467e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataModule to create the datasets and the dataloaders                                                                                                                                                        \n",
    "class SpeechDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,train_dataset, test_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        self.train_dataset = self.train_dataset\n",
    "        self.test_dataset = self.test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, shuffle = True, batch_size = self.batch_size, num_workers = 16, collate_fn=PadImage_inf_comp())\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = len(self.test_dataset), shuffle = False, num_workers = 16, collate_fn=PadImage_inf_comp_val())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the module                                                                                                                                                                                             \n",
    "train_dataset = SpeechDataset(train_csv_file, train_demo_csv_file, root_dir, transforms_train)\n",
    "test_dataset = SpeechDataset_val(val_csv_file, val_demo_csv_file, root_dir, transforms_val)\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "data_module = SpeechDataModule(train_dataset, test_dataset, Batch_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data_module.val_dataloader()))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e27fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[10][2][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[43][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(train_dataset[10][0][0].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f56d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(train_dataset[10][0][1].numpy().T, x_axis='time', sr=8000,hop_length= 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fedb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(test_dataset[100][0][0].numpy().T, x_axis='time', sr=8000, hop_length= 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9ca7e",
   "metadata": {},
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "max_ = -10000\n",
    "min_ = 10000\n",
    "for data in data_module.train_dataloader():\n",
    "    data = data[0]\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    if data.max() > max_:\n",
    "        max_ = data.max()\n",
    "        \n",
    "    if data.min() < min_:\n",
    "        min_ = data.min()\n",
    "        \n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "print(mean)\n",
    "print(std)\n",
    "print(max_)\n",
    "print(min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfab226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor class performing all the calculations for loss, backpropagation etc                                                                                                                               \n",
    "class Speech_Predictor(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Speech_Predictor,self).__init__()\n",
    "        self.n_classes = 2\n",
    "        self.model = FC_Resnet_comp_(num_layers = 2, num_classes = 2)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        self.train_f1 = torchmetrics.F1(num_classes = self.n_classes, average = 'weighted')\n",
    "        #self.valid_f1 = torchmetrics.F1(num_classes = self.n_classes, average = 'weighted')                                                                                                                  \n",
    "\n",
    "        self.train_f1_class = torchmetrics.F1(num_classes = self.n_classes, average = None)\n",
    "        #self.valid_f1_class = torchmetrics.F1(num_classes = self.n_classes, average = None)                                                                                                                  \n",
    "\n",
    "        self.train_auc_class = torchmetrics.AUROC(num_classes = self.n_classes, average = None)\n",
    "        #self.valid_auc_class = torchmetrics.AUROC(num_classes = self.n_classes, average = None)                                                                                                              \n",
    "\n",
    "        self.Spearman = torchmetrics.SpearmanCorrcoef()\n",
    "\n",
    "    def forward(self,x, x_comp = None, labels = None):\n",
    "\n",
    "        output = self.model(x, x_comp)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output,labels)\n",
    "            return loss, output\n",
    "        else:\n",
    "            #output = F.softmax(output,dim =1)\n",
    "            return output\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        X = batch[0]\n",
    "        X_comp = batch[1]\n",
    "        y = batch[2]\n",
    "\n",
    "        loss, outputs = self(x = X, x_comp = X_comp, labels = y)\n",
    "\n",
    "        outputs = F.softmax(outputs,dim =1)\n",
    "        yhat = torch.argmax(outputs, dim =1)\n",
    "\n",
    "        train_f1 = self.train_f1(yhat, y)\n",
    "        train_f1_class = self.train_f1_class(yhat, y)\n",
    "\n",
    "        self.log(\"train_loss\",loss,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_f1\",train_f1,prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_f1_less\",train_f1_class[0],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_f1_more\",train_f1_class[1],prog_bar = False, logger = True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        X = batch[0]\n",
    "        bars = batch[1]\n",
    "        i_d = batch[2]\n",
    "\n",
    "        outputs = self(x = X, x_comp = None, labels = None)\n",
    "        #outputs = F.softmax(outputs,dim =1)\n",
    "        outputs, _ = groupby_mean(outputs, i_d)\n",
    "\n",
    "        bars, bars_index = groupby_mean(bars.view((bars.shape[0],1)), i_d)\n",
    "        bars = bars.type(torch.DoubleTensor).to(device)\n",
    "\n",
    "        valid_Spearman_0 = self.Spearman(outputs[:,0], bars[:,0])\n",
    "        valid_Spearman_1 = self.Spearman(outputs[:,1], bars[:,0])\n",
    "        valid_Spearman_2 = self.Spearman(torch.maximum(outputs[:,0], outputs[:,1]), bars[:,0])\n",
    "        valid_Spearman_3 = self.Spearman(torch.minimum(outputs[:,0], outputs[:,1]), bars[:,0])\n",
    "\n",
    "\n",
    "        self.log(\"Spearman_coeff_0\",valid_Spearman_0, prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"Spearman_coeff_1\",valid_Spearman_1, prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"Spearman_coeff_2\",valid_Spearman_2, prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "        self.log(\"Spearman_coeff_3\",valid_Spearman_3, prog_bar = True, logger = True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr =1.e-5, weight_decay=1e-2)\n",
    "\n",
    "        return [optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model       \n",
    "model = Speech_Predictor()\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint and loger definition                                                                                                                                                                              \n",
    "checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_directory,filename='Resnet_best-checkpoint-{epoch:02d}-{Spearman_coeff_1_epoch:.4f}_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fre\\\n",
    "sh_v'+str(fold),save_top_k=3, verbose =True , monitor = 'Spearman_coeff_1_epoch',mode ='max')\n",
    "logger = TensorBoardLogger(parent_directory + 'lightning_logs', name = 'Resnet_regression_speech_control_AT_Mel_grad_tf_10fold_fresh_comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_on is True:\n",
    "    #Defining the trainer object\n",
    "    trainer = pl.Trainer(logger = logger, callbacks = [checkpoint_callback], max_epochs = epochs, gpus = 1)\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e05f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9394a28",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models\n",
    "'''\n",
    "checkpoint_loc_v0 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=93-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v0.ckpt'\n",
    "checkpoint_loc_v1 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=41-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v1.ckpt'\n",
    "checkpoint_loc_v2 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=64-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v2.ckpt'\n",
    "checkpoint_loc_v3 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=57-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v3.ckpt'\n",
    "checkpoint_loc_v4 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=94-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v4.ckpt'\n",
    "checkpoint_loc_v5 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=38-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v5.ckpt'\n",
    "checkpoint_loc_v6 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=95-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v6.ckpt'\n",
    "checkpoint_loc_v7 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=29-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v7.ckpt'\n",
    "checkpoint_loc_v8 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=75-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v8.ckpt'\n",
    "checkpoint_loc_v9 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=19-Spearman_coeff_1_epoch=0.0000_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v9.ckpt'\n",
    "\n",
    "checkpoint_loc_v0 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=99-Spearman_coeff_1_epoch=0.8054_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v0.ckpt'\n",
    "checkpoint_loc_v1 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=92-Spearman_coeff_1_epoch=0.8909_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v1.ckpt'\n",
    "checkpoint_loc_v2 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=98-Spearman_coeff_1_epoch=0.8179_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v2.ckpt'\n",
    "checkpoint_loc_v3 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=88-Spearman_coeff_1_epoch=0.7926_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v3.ckpt'\n",
    "checkpoint_loc_v4 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=99-Spearman_coeff_1_epoch=0.8110_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v4.ckpt'\n",
    "checkpoint_loc_v5 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=93-Spearman_coeff_1_epoch=0.7381_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v5.ckpt'\n",
    "checkpoint_loc_v6 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=74-Spearman_coeff_1_epoch=0.4635_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v6.ckpt'\n",
    "checkpoint_loc_v7 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=91-Spearman_coeff_1_epoch=0.6576_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v7.ckpt'\n",
    "checkpoint_loc_v8 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=86-Spearman_coeff_1_epoch=0.6067_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v8.ckpt'\n",
    "checkpoint_loc_v9 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=57-Spearman_coeff_1_epoch=0.7266_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v9.ckpt'\n",
    "'''\n",
    "checkpoint_loc_v0 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=54-Pearson_coeff_1=0.8011_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v0.ckpt'\n",
    "checkpoint_loc_v1 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=82-Pearson_coeff_1=0.8807_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v1.ckpt'\n",
    "checkpoint_loc_v2 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=93-Pearson_coeff_1=0.7907_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v2.ckpt'\n",
    "checkpoint_loc_v3 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=85-Pearson_coeff_1=0.9184_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v3.ckpt'\n",
    "checkpoint_loc_v4 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=31-Pearson_coeff_1=0.8031_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v4.ckpt'\n",
    "checkpoint_loc_v5 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=79-Pearson_coeff_1=0.8249_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v5.ckpt'\n",
    "checkpoint_loc_v6 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=60-Pearson_coeff_1=0.6092_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v6.ckpt'\n",
    "checkpoint_loc_v7 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=97-Pearson_coeff_1=0.7368_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v7.ckpt'\n",
    "checkpoint_loc_v8 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=45-Pearson_coeff_1=0.7450_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v8.ckpt'\n",
    "checkpoint_loc_v9 = checkpoint_directory + 'Resnet_best-checkpoint-epoch=15-Pearson_coeff_1=0.6600_control_AT_regression__speech_mel_grad_tf_10fold_comparisons_fresh_nc_v9.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "trained_model_v0 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v0)\n",
    "trained_model_v1 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v1)\n",
    "trained_model_v2 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v2)\n",
    "trained_model_v3 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v3)\n",
    "trained_model_v4 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v4)\n",
    "trained_model_v5 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v5)\n",
    "trained_model_v6 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v6)\n",
    "trained_model_v7 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v7)\n",
    "trained_model_v8 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v8)\n",
    "trained_model_v9 = Speech_Predictor.load_from_checkpoint(checkpoint_loc_v9)\n",
    "\n",
    "trained_model_v0.freeze()\n",
    "trained_model_v0.double()\n",
    "trained_model_v1.freeze()\n",
    "trained_model_v1.double()\n",
    "trained_model_v2.freeze()\n",
    "trained_model_v2.double()\n",
    "trained_model_v3.freeze()\n",
    "trained_model_v3.double()\n",
    "trained_model_v4.freeze()\n",
    "trained_model_v4.double()\n",
    "trained_model_v5.freeze()\n",
    "trained_model_v5.double()\n",
    "trained_model_v6.freeze()\n",
    "trained_model_v6.double()\n",
    "trained_model_v7.freeze()\n",
    "trained_model_v7.double()\n",
    "trained_model_v8.freeze()\n",
    "trained_model_v8.double()\n",
    "trained_model_v9.freeze()\n",
    "trained_model_v9.double()\n",
    "models = [trained_model_v0, trained_model_v1, trained_model_v2, trained_model_v3, trained_model_v4,\n",
    "          trained_model_v5, trained_model_v6, trained_model_v7, trained_model_v8, trained_model_v9]\n",
    "\n",
    "\n",
    "#Demographics files\n",
    "\n",
    "val_demo_v0 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v0.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\",\"Bars_Speech\",\"Date\"])\n",
    "val_demo_v1 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v1.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v2 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v2.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v3 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v3.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v4 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v4.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v5 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v5.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\",\"Bars_Speech\",\"Date\"])\n",
    "val_demo_v6 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v6.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v7 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v7.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v8 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v8.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_v9 = pd.read_csv(root_dir + 'val_demo_Mel_severity_cnn_nr2_v9.csv', names=[\"No\",\"P_ID\", \"Sex\", \"Bars\",\"Age\", \"Bars_Speech\",\"Date\"])\n",
    "val_demo_ = [val_demo_v0, val_demo_v1, val_demo_v2, val_demo_v3, val_demo_v4,\n",
    "             val_demo_v5, val_demo_v6, val_demo_v7, val_demo_v8, val_demo_v9]\n",
    "\n",
    "#All validation data sets \n",
    "\n",
    "val_csv_file_v0 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v0.csv'\n",
    "val_csv_file_v1 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v1.csv'\n",
    "val_csv_file_v2 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v2.csv'\n",
    "val_csv_file_v3 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v3.csv'\n",
    "val_csv_file_v4 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v4.csv'\n",
    "val_csv_file_v5 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v5.csv'\n",
    "val_csv_file_v6 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v6.csv'\n",
    "val_csv_file_v7 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v7.csv'\n",
    "val_csv_file_v8 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v8.csv'\n",
    "val_csv_file_v9 = root_dir + 'val_dataset_control_AT_Mel_Spec_2022_noise_red2_severity_v9.csv'\n",
    "\n",
    "test_dataset_v0 = SpeechDataset_val(val_csv_file_v0, root_dir + 'val_demo_Mel_severity_cnn_nr2_v0.csv', root_dir,transforms_val)\n",
    "test_dataset_v1 = SpeechDataset_val(val_csv_file_v1, root_dir + 'val_demo_Mel_severity_cnn_nr2_v1.csv', root_dir,transforms_val)\n",
    "test_dataset_v2 = SpeechDataset_val(val_csv_file_v2, root_dir + 'val_demo_Mel_severity_cnn_nr2_v2.csv', root_dir,transforms_val)\n",
    "test_dataset_v3 = SpeechDataset_val(val_csv_file_v3, root_dir + 'val_demo_Mel_severity_cnn_nr2_v3.csv', root_dir,transforms_val)\n",
    "test_dataset_v4 = SpeechDataset_val(val_csv_file_v4, root_dir + 'val_demo_Mel_severity_cnn_nr2_v4.csv', root_dir,transforms_val)\n",
    "test_dataset_v5 = SpeechDataset_val(val_csv_file_v5, root_dir + 'val_demo_Mel_severity_cnn_nr2_v5.csv', root_dir,transforms_val)\n",
    "test_dataset_v6 = SpeechDataset_val(val_csv_file_v6, root_dir + 'val_demo_Mel_severity_cnn_nr2_v6.csv', root_dir,transforms_val)\n",
    "test_dataset_v7 = SpeechDataset_val(val_csv_file_v7, root_dir + 'val_demo_Mel_severity_cnn_nr2_v7.csv', root_dir,transforms_val)\n",
    "test_dataset_v8 = SpeechDataset_val(val_csv_file_v8, root_dir + 'val_demo_Mel_severity_cnn_nr2_v8.csv', root_dir,transforms_val)\n",
    "test_dataset_v9 = SpeechDataset_val(val_csv_file_v9, root_dir + 'val_demo_Mel_severity_cnn_nr2_v9.csv', root_dir,transforms_val)\n",
    "all_data = [test_dataset_v0, test_dataset_v1, test_dataset_v2, test_dataset_v3, test_dataset_v4,\n",
    "            test_dataset_v5, test_dataset_v6, test_dataset_v7, test_dataset_v8, test_dataset_v9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827dcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the output of the models\n",
    "val_demo__ = []\n",
    "for i in range(10):\n",
    "    test_dataset = all_data[i]\n",
    "    trained_model = models[i]\n",
    "    val_demo = val_demo_[i]\n",
    "    bars_pred = []\n",
    "    y_label_list =[]\n",
    "    bars_obsr_list =[]\n",
    "    date_list = []\n",
    "    for sample in test_dataset:\n",
    "        X_s, bars,date_, y_label = sample\n",
    "        input_ = X_s.double()\n",
    "        input_ = torch.unsqueeze(input_, 0) \n",
    "        output = trained_model(input_)\n",
    "        val_demo.loc[(val_demo.Date == date_[0].detach().cpu().numpy()),'Label'] = y_label[0].detach().cpu().numpy()\n",
    "        bars_pred.append(output[0][1].detach().cpu().numpy())\n",
    "        bars_obsr_list.append(bars[0].detach().cpu().numpy())\n",
    "        #y_label_list.append(y_label[0].detach().cpu().numpy())\n",
    "        #date_list.append(date_[0].detach().cpu().numpy())\n",
    "    \n",
    "    val_demo.loc[(val_demo.Label == 0),'Bars_Speech']= 0.\n",
    "    val_demo = val_demo[val_demo['Bars_Speech'].notna()]\n",
    "    val_demo = val_demo[val_demo['Bars_Speech'] >= 0 ]\n",
    "    val_demo.loc[val_demo['Bars_Speech'].notna(), \"BARS_pred\"] = bars_pred\n",
    "    val_demo.loc[val_demo['Bars_Speech'].notna(), \"BARS_obsr\"] = bars_obsr_list\n",
    "    val_demo__.append(val_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_demo__ = [val_demo_[0]]#[val_demo_[0], val_demo_[1], val_demo_[2], val_demo_[3], val_demo_[4],\n",
    "              #val_demo_[5], val_demo_[6], val_demo_[7], val_demo_[8], val_demo_[9]]\n",
    "val_demo_all = pd.concat(val_demo__, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "#val_demo_bars = val_demo_bars[val_demo_bars['Label'] == 1]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'] >= 0]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars = val_demo_bars[['P_ID','Label', 'Age','BARS_obsr', 'BARS_pred', 'Date']].astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).mean()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "cmap = plt.get_cmap('gray')\n",
    "new_cmap = truncate_colormap(cmap, 0.2, 1)\n",
    "\n",
    "val_demo_bars_ = pd.concat([val_demo_bars_male, val_demo_bars_female], ignore_index=True)\n",
    "#sns.kdeplot(x=val_demo_bars_['BARS_obsr'], y=val_demo_bars_['BARS_pred'], cmap=new_cmap, shade=True, bw_adjust=.65, clip=([-0.5,30],[-0.5, 30.0]))\n",
    "\n",
    "#ax.scatter(val_demo_bars_male['BARS_obsr'], val_demo_bars_male['BARS_pred'], c = 'red', marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "#ax.scatter(val_demo_bars_female['BARS_obsr'], val_demo_bars_female['BARS_pred'], c = 'red', marker = 'x', s = 2* val_demo_bars_female['Age'])\n",
    "#ax.plot([0, 4], [0, 4],color = 'k',linewidth = 5,linestyle ='-.')\n",
    "\n",
    "#ax.fill_between([0,4], [-0.43, 4 -0.43], [0.43, 4 + 0.43], color='k', alpha=.1)\n",
    "\n",
    "z, V = np.polyfit(val_demo_bars['BARS_obsr'], val_demo_bars['BARS_pred'], 1, cov=True)\n",
    "p = np.poly1d(z)\n",
    "slope_err = np.sqrt(V[0][0])\n",
    "inter_err = np.sqrt(V[1][1])\n",
    "\n",
    "plt.plot(range(5),p(range(5)),\"k\",linewidth = 3)\n",
    "\n",
    "#ax.fill_between([0,4], [p(0)-inter_err , p(4) - 4 * slope_err - inter_err], [p(0)+inter_err , p(4) + 4 * slope_err + inter_err], color='k', alpha=.1)\n",
    "\n",
    "###############################33\n",
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Label'] == 1]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'] >= 0]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars = val_demo_bars[['P_ID','Label', 'Age','BARS_obsr', 'BARS_pred', 'Date']].astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).mean()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "cmap = plt.get_cmap('gray')\n",
    "new_cmap = truncate_colormap(cmap, 0.2, 1)\n",
    "\n",
    "val_demo_bars_ = pd.concat([val_demo_bars_male, val_demo_bars_female], ignore_index=True)\n",
    "#sns.kdeplot(x=val_demo_bars_['BARS_obsr'], y=val_demo_bars_['BARS_pred'], cmap=new_cmap, shade=True, bw_adjust=.65, clip=([-0.5,30],[-0.5, 30.0]))\n",
    "\n",
    "ax.scatter(val_demo_bars_male['BARS_obsr'], val_demo_bars_male['BARS_pred'], c = 'red', marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "ax.scatter(val_demo_bars_female['BARS_obsr'], val_demo_bars_female['BARS_pred'], c = 'red', marker = 'x', s = 2* val_demo_bars_female['Age'])\n",
    "\n",
    "\n",
    "#########################################\n",
    "\n",
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Label'] == 0]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'] >= 0]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars = val_demo_bars[['P_ID','Label', 'Age','BARS_obsr', 'BARS_pred', 'Date']].astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).mean()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "cmap = plt.get_cmap('gray')\n",
    "new_cmap = truncate_colormap(cmap, 0.2, 1)\n",
    "\n",
    "val_demo_bars_ = pd.concat([val_demo_bars_male, val_demo_bars_female], ignore_index=True)\n",
    "#sns.kdeplot(x=val_demo_bars_['BARS_obsr'], y=val_demo_bars_['BARS_pred'], cmap=new_cmap, shade=True, bw_adjust=.65, clip=([-0.5,30],[-0.5, 30.0]))\n",
    "\n",
    "ax.scatter(val_demo_bars_male['BARS_obsr'], val_demo_bars_male['BARS_pred'], c = 'blue', marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "ax.scatter(val_demo_bars_female['BARS_obsr'], val_demo_bars_female['BARS_pred'], c = 'blue', marker = 'x', s = 2* val_demo_bars_female['Age'])\n",
    "\n",
    "ax.minorticks_on()\n",
    "ax.tick_params('both', length=10, width=2, which='major',direction=\"in\")\n",
    "ax.tick_params('both', length=5, width=1, which='minor',direction=\"in\")\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "#ax.set_aspect(1)\n",
    "plt.xlim([-0.15, 4])\n",
    "#plt.ylim([-1, 1])\n",
    "plt.xlabel(r'$BARS^{clin}_{speech}$', fontsize=40)\n",
    "plt.ylabel(r'$Severity \\ Score$', fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "#val_demo_bars = val_demo_bars[val_demo_bars['Label'] == 1]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'] >= 0]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars = val_demo_bars[['P_ID','Label', 'Age','BARS_obsr', 'BARS_pred', 'Date']].astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).mean()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "cmap = plt.get_cmap('gray')\n",
    "new_cmap = truncate_colormap(cmap, 0.2, 1)\n",
    "\n",
    "val_demo_bars_ = pd.concat([val_demo_bars_male, val_demo_bars_female], ignore_index=True)\n",
    "\n",
    "df = pd.DataFrame({'X':val_demo_bars_['BARS_obsr'], 'H': val_demo_bars_['BARS_pred']})\n",
    "df.boxplot(by='X', ax=ax)\n",
    "\n",
    "ax.minorticks_on()\n",
    "ax.tick_params('both', length=10, width=2, which='major',direction=\"in\")\n",
    "ax.tick_params('both', length=5, width=1, which='minor',direction=\"in\")\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "#ax.set_aspect(1)\n",
    "#plt.xlim([-0.15, 4])\n",
    "#plt.ylim([-1, 1])\n",
    "plt.xlabel(r'$BARS^{clin}_{speech}$', fontsize=40)\n",
    "plt.ylabel(r'$Severity \\ Score$', fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef43d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(val_demo_bars['BARS_obsr'], val_demo_bars['BARS_pred'],squared = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(val_demo_bars['BARS_obsr'], val_demo_bars['BARS_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53803386",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(val_demo_bars['BARS_obsr'], val_demo_bars['BARS_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(val_demo_bars['BARS_obsr'], val_demo_bars['BARS_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'] >= 0]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Label'] == 1]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars = val_demo_bars[['P_ID','Label', 'Age','BARS_obsr', 'BARS_pred', 'Date']].astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).median()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "\n",
    "male_month_diff_1 = []\n",
    "male_month_diff_2 = []\n",
    "male_month_diff_3 = []\n",
    "male_month_diff_4 = []\n",
    "male_bars_diff_1 = []\n",
    "male_bars_diff_2 = []\n",
    "male_bars_diff_3 = []\n",
    "male_bars_diff_4 = []\n",
    "male_init_bars = []\n",
    "female_month_diff_1 = []\n",
    "female_month_diff_2 = []\n",
    "female_month_diff_3 = []\n",
    "female_month_diff_4 = []\n",
    "female_bars_diff_1 = []\n",
    "female_bars_diff_2 = []\n",
    "female_bars_diff_3 = []\n",
    "female_bars_diff_4 = []\n",
    "female_init_bars = []\n",
    "\n",
    "#print(val_demo_bars_male['Date'].astype(str).str[5:9].astype(float)*12. + val_demo_bars_male['Date'].astype(str).str[9:11].astype(float))\n",
    "for vv in val_demo_bars_male['ID_ranked'].unique():\n",
    "    if val_demo_bars_male['ID_ranked'].value_counts().loc[vv] > 1:\n",
    "        val_demo_bars_male_lines = val_demo_bars_male[val_demo_bars_male['ID_ranked'] == vv]\n",
    "        val_demo_bars_male_lines = val_demo_bars_male_lines.sort_values(by=['Date'])\n",
    "        val_demo_bars_male_lines['Months'] = val_demo_bars_male_lines['Date'].astype(str).str[5:9].astype(float)*12. + val_demo_bars_male_lines['Date'].astype(str).str[9:11].astype(float)\n",
    "        month_diff = val_demo_bars_male_lines['Months'].iloc[-1]- val_demo_bars_male_lines['Months'].iloc[0]\n",
    "        if month_diff > 1:\n",
    "            male_init_bars.append(val_demo_bars_male_lines['BARS_obsr'].iloc[0])\n",
    "            Bars_pred_diff = val_demo_bars_male_lines['BARS_pred'].iloc[-1]- val_demo_bars_male_lines['BARS_pred'].iloc[0]\n",
    "            Bars_obs_diff = val_demo_bars_male_lines['BARS_obsr'].iloc[-1]- val_demo_bars_male_lines['BARS_obsr'].iloc[0]\n",
    "            if Bars_obs_diff == 0.:\n",
    "                male_bars_diff_1.append(Bars_pred_diff)\n",
    "                male_month_diff_1.append(month_diff)\n",
    "            elif ((Bars_obs_diff > 0.0) and (Bars_obs_diff < 0.75)):\n",
    "                male_bars_diff_2.append(Bars_pred_diff)\n",
    "                male_month_diff_2.append(month_diff)\n",
    "            else:\n",
    "                male_bars_diff_3.append(Bars_pred_diff)\n",
    "                male_month_diff_3.append(month_diff)\n",
    "        if Bars_pred_diff < 0:\n",
    "                print(vv)\n",
    "            \n",
    "for vv in val_demo_bars_female['ID_ranked'].unique():\n",
    "    if val_demo_bars_female['ID_ranked'].value_counts().loc[vv] > 1:\n",
    "        val_demo_bars_female_lines = val_demo_bars_female[val_demo_bars_female['ID_ranked'] == vv]\n",
    "        val_demo_bars_female_lines = val_demo_bars_female_lines.sort_values(by=['Date'])\n",
    "        val_demo_bars_female_lines['Months'] = val_demo_bars_female_lines['Date'].astype(str).str[5:9].astype(float)*12. + val_demo_bars_female_lines['Date'].astype(str).str[9:11].astype(float)\n",
    "        month_diff = val_demo_bars_female_lines['Months'].iloc[-1]- val_demo_bars_female_lines['Months'].iloc[0]\n",
    "        if month_diff > 1:\n",
    "            female_init_bars.append(val_demo_bars_female_lines['BARS_obsr'].iloc[0])\n",
    "            Bars_pred_diff = val_demo_bars_female_lines['BARS_pred'].iloc[-1]- val_demo_bars_female_lines['BARS_pred'].iloc[0]\n",
    "            Bars_obs_diff = val_demo_bars_female_lines['BARS_obsr'].iloc[-1]- val_demo_bars_female_lines['BARS_obsr'].iloc[0]\n",
    "            if Bars_obs_diff == 0:\n",
    "                female_bars_diff_1.append(Bars_pred_diff)\n",
    "                female_month_diff_1.append(month_diff)\n",
    "            elif ((Bars_obs_diff > 0) and (Bars_obs_diff < 0.75)):\n",
    "                female_bars_diff_2.append(Bars_pred_diff)\n",
    "                female_month_diff_2.append(month_diff)\n",
    "            else:\n",
    "                female_bars_diff_3.append(Bars_pred_diff)\n",
    "                female_month_diff_3.append(month_diff)\n",
    "            if Bars_pred_diff < 0:\n",
    "                print(vv)\n",
    "                \n",
    "ax.scatter(male_month_diff_1, male_bars_diff_1, c = 'red', marker = 'o',s = 90)#10*np.array(male_init_bars))\n",
    "ax.scatter(male_month_diff_2, male_bars_diff_2, c = 'blue', marker = 'o',s = 90)#10*np.array(male_init_bars))\n",
    "ax.scatter(male_month_diff_3, male_bars_diff_3, c = 'green', marker = 'o',s = 90)#10*np.array(male_init_bars))\n",
    "#ax.scatter(male_month_diff_4, male_bars_diff_4, c = 'orange', marker = 'o',s = 70)#10*np.array(male_init_bars))\n",
    "\n",
    "ax.scatter(female_month_diff_1, female_bars_diff_1, c = 'red', marker = 'o',s = 90)#10*np.array(male_init_bars))\n",
    "ax.scatter(female_month_diff_2, female_bars_diff_2, c = 'blue', marker = 'o',s = 90)#10*np.array(male_init_bars))\n",
    "ax.scatter(female_month_diff_3, female_bars_diff_3, c = 'green', marker = 'o',s = 90)#10*np.array(male_init_bars))\n",
    "#ax.scatter(female_month_diff_4, female_bars_diff_4, c = 'orange', marker = 'x',s = 70)#10*np.array(male_init_bars))\n",
    "\n",
    "print(np.mean(male_bars_diff_1 + male_bars_diff_2 + male_bars_diff_3 + female_bars_diff_1 + female_bars_diff_2 + female_bars_diff_3 ), np.std(male_bars_diff_1 + male_bars_diff_2 + male_bars_diff_3 + female_bars_diff_1 + female_bars_diff_2 + female_bars_diff_3 ))\n",
    "\n",
    "ax.scatter([], [], c = 'red', marker = 'o',s = 90, label=r'$\\rm{\\Delta BARS^{clin}_{speech}} = 0$')#10*np.array(male_init_bars))\n",
    "ax.scatter([], [], c = 'blue', marker = 'o',s = 90, label=r'$0 < \\rm{\\Delta BARS^{clin}_{speech}} < 0.75$')#10*np.array(male_init_bars))\n",
    "ax.scatter([], [], c = 'green', marker = 'o',s = 90, label=r'$\\rm{\\Delta BARS^{clin}_{speech}}$ > 0.75')#10*np.array(male_init_bars))\n",
    "#plt.legend(fontsize=25,loc = 'lower right')\n",
    "ax.minorticks_on()\n",
    "ax.tick_params('both', length=10, width=2, which='major',direction=\"in\")\n",
    "ax.tick_params('both', length=5, width=1, which='minor',direction=\"in\")\n",
    "\n",
    "plt.axhline(y=0., color='k', linestyle='--')\n",
    "#plt.axhline(y=0.75, color='k', linestyle='--')\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "#ax.set_aspect(12.5)\n",
    "#plt.xlim([-0.2, 45])\n",
    "#plt.ylim([-1.7, 1.7])\n",
    "plt.xlabel(r'$\\rm{Months}$', fontsize=40)\n",
    "plt.ylabel(r'$Severity \\ Score$', fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_diff =  male_month_diff_1 + male_month_diff_2 + male_month_diff_3 + female_month_diff_1 + female_month_diff_2 + female_month_diff_3 \n",
    "bars_diff = male_bars_diff_1 + male_bars_diff_2 + male_bars_diff_3 + female_bars_diff_1 + female_bars_diff_2 + female_bars_diff_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13903a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr( month_diff, bars_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b143227",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(bars_diff, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'] >= 0]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars = val_demo_bars[['P_ID','Label', 'Age','BARS_obsr', 'BARS_pred', 'Date']].astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['ID_ranked','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).median()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).median()\n",
    "\n",
    "val_demo_bars_male['MAE'] = np.abs(val_demo_bars_male['BARS_obsr'] - val_demo_bars_male['BARS_pred'])\n",
    "val_demo_bars_female['MAE'] = np.abs(val_demo_bars_female['BARS_obsr'] - val_demo_bars_female['BARS_pred'])\n",
    "\n",
    "val_demo_bars_male_np_control = val_demo_bars_male[(val_demo_bars_male['Label'] == 0) ]['MAE'].to_numpy()\n",
    "val_demo_bars_female_np_control = val_demo_bars_female[(val_demo_bars_female['Label'] == 0)]['MAE'].to_numpy()\n",
    "\n",
    "val_demo_bars_male_np_ataxia = val_demo_bars_male[(val_demo_bars_male['Label'] == 1) ]['MAE'].to_numpy()\n",
    "val_demo_bars_female_np_ataxia = val_demo_bars_female[(val_demo_bars_female['Label'] == 1)]['MAE'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(a=val_demo_bars_male_np_control, b=val_demo_bars_female_np_control, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d276c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(a=val_demo_bars_male_np_ataxia, b=val_demo_bars_female_np_ataxia, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520982d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date']]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['BARS_obsr'] >= 0]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Label'] == 1]\n",
    "val_demo_bars['MAE'] = np.abs(val_demo_bars['BARS_obsr'] - val_demo_bars['BARS_pred'])\n",
    "val_demo_bars.loc[(val_demo_bars.Label == 0),'Bars']= 0.\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'M'),'Sex']= 0\n",
    "val_demo_bars.loc[(val_demo_bars.Sex == 'F'),'Sex']= 1\n",
    "val_demo_bars[\"ID_ranked\"] = val_demo_bars[\"P_ID\"].rank()-1\n",
    "val_demo_bars = val_demo_bars.astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date',\"Age\"], as_index = False).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_demo_bars[['MAE','Age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(val_demo_bars['Age'], val_demo_bars['MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c3d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "val_demo_bars = val_demo_all[['P_ID','Sex','Label', 'Age','BARS_obsr', 'BARS_pred','Date','Bars']]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars'].notna()]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Bars'] >= 0]\n",
    "val_demo_bars = val_demo_bars[val_demo_bars['Label'] == 1]\n",
    "val_demo_bars_male = val_demo_bars[val_demo_bars['Sex'] == \"M\"]\n",
    "val_demo_bars_female = val_demo_bars[val_demo_bars['Sex'] == \"F\"]\n",
    "val_demo_bars_male[\"ID_ranked\"] = val_demo_bars_male[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars_female[\"ID_ranked\"] = val_demo_bars_female[\"P_ID\"]#.rank()-1\n",
    "val_demo_bars = val_demo_bars[['P_ID','Label', 'Age','Bars', 'BARS_pred', 'Date']].astype('float64')\n",
    "val_demo_bars = val_demo_bars.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male[['ID_ranked','Label', 'Age','Bars', 'BARS_pred','Date']]\n",
    "val_demo_bars_male = val_demo_bars_male.astype('float64')\n",
    "\n",
    "val_demo_bars_female = val_demo_bars_female[['ID_ranked','Label', 'Age','Bars', 'BARS_pred','Date']]\n",
    "val_demo_bars_female = val_demo_bars_female.astype('float64')\n",
    "\n",
    "\n",
    "val_demo_bars_male_lower68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "val_demo_bars_female_lower68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.159)\n",
    "\n",
    "val_demo_bars_male_upper68 = val_demo_bars_male.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "val_demo_bars_female_upper68 = val_demo_bars_female.groupby(['Date'], as_index = False).quantile(0.841)\n",
    "\n",
    "val_demo_bars_male = val_demo_bars_male.groupby(['Date'], as_index = False).mean()\n",
    "val_demo_bars_female = val_demo_bars_female.groupby(['Date'], as_index = False).mean()\n",
    "\n",
    "cmap = plt.get_cmap('gray')\n",
    "new_cmap = truncate_colormap(cmap, 0.2, 1)\n",
    "\n",
    "val_demo_bars_ = pd.concat([val_demo_bars_male, val_demo_bars_female], ignore_index=True)\n",
    "#sns.kdeplot(x=val_demo_bars_['BARS_obsr'], y=val_demo_bars_['BARS_pred'], cmap=new_cmap, shade=True, bw_adjust=.65, clip=([-0.5,30],[-0.5, 30.0]))\n",
    "\n",
    "ax.scatter(val_demo_bars_male['Bars'], val_demo_bars_male['BARS_pred'], c = 'red', marker = 'o', s = 2*val_demo_bars_male['Age'])\n",
    "ax.scatter(val_demo_bars_female['Bars'], val_demo_bars_female['BARS_pred'], c = 'red', marker = 'x', s = 2* val_demo_bars_female['Age'])\n",
    "#ax.plot([0, 4], [0, 4],color = 'k',linewidth = 5,linestyle ='-.')\n",
    "\n",
    "#ax.fill_between([0,4], [-0.43, 4 -0.43], [0.43, 4 + 0.43], color='k', alpha=.1)\n",
    "\n",
    "z, V = np.polyfit(val_demo_bars['Bars'], val_demo_bars['BARS_pred'], 1, cov=True)\n",
    "p = np.poly1d(z)\n",
    "slope_err = np.sqrt(V[0][0])\n",
    "inter_err = np.sqrt(V[1][1])\n",
    "plt.plot(range(31),p(range(31)),\"k\",linewidth = 3)\n",
    "\n",
    "#ax.fill_between([0,4], [p(0)-inter_err , p(4) - 4 * slope_err - inter_err], [p(0)+inter_err , p(4) + 4 * slope_err + inter_err], color='k', alpha=.1)\n",
    "\n",
    "\n",
    "\n",
    "ax.minorticks_on()\n",
    "ax.tick_params('both', length=10, width=2, which='major',direction=\"in\")\n",
    "ax.tick_params('both', length=5, width=1, which='minor',direction=\"in\")\n",
    "ax.tick_params(axis='x', labelsize=35)\n",
    "ax.tick_params(axis='y', labelsize=35)\n",
    "#ax.set_aspect(1)\n",
    "#plt.xlim([-0.15, 4])\n",
    "#plt.ylim([0, 1])\n",
    "plt.xlabel(r'$BARS^{clin}_{total}$', fontsize=40)\n",
    "plt.ylabel(r'$Severity \\ Score$', fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(val_demo_bars['Bars'], val_demo_bars['BARS_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe9b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ccaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4c126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
